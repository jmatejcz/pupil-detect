{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.PupilCoreDatasetIfOpened import PupilCoreDatasetIfOpened\n",
    "from datasets.PupilCoreDatasetCoords import PupilCoreDatasetCoords\n",
    "from models.ifOpened import ifOpenedModel\n",
    "from models.pupilDetectModel import PupilDetectModel\n",
    "from models.utils import train_first_model\n",
    "import models.utils as utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "from torchvision import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = PupilCoreDatasetIfOpened(\n",
    "#     \"datasets/PupilCoreDataset/video5_eye0_video.avi\",\n",
    "#     'datasets/PupilCoreDataset/video5_eye0_pupildata.csv',\n",
    "#     \"datasets/PupilCoreDataset/video5_eye1_video.avi\",\n",
    "#     'datasets/PupilCoreDataset/video5_eye1_pupildata.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# train_part = int(0.8 * len(dataset))\n",
    "# train_dataset = torch.utils.data.Subset(dataset, indices[:train_part])\n",
    "# test_dataset = torch.utils.data.Subset(dataset, indices[train_part:])\n",
    "# dataset_sizes = {\n",
    "#     'train': len(train_dataset),\n",
    "#     'test': len(test_dataset)\n",
    "# }\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "# dataloaders = {\n",
    "#     \"train\": train_dataloader,\n",
    "#     \"test\": test_dataloader\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ifOpenedModel()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# model = train_first_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=2, dataloaders=dataloaders, dataset_sizes=dataset_sizes, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\inzynierka\\my_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Desktop\\inzynierka\\my_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model for datasetCoords.py\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(in_features=512, out_features=2, dtype=torch.float32)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't receive frame\n",
      "can't receive frame\n",
      "(tensor([[[0.4824, 0.4824, 0.4863,  ..., 0.5686, 0.5569, 0.5529],\n",
      "         [0.4824, 0.4863, 0.4863,  ..., 0.5686, 0.5608, 0.5569],\n",
      "         [0.4863, 0.4863, 0.4863,  ..., 0.5686, 0.5608, 0.5569],\n",
      "         ...,\n",
      "         [0.7176, 0.7216, 0.7255,  ..., 0.5412, 0.5333, 0.5294],\n",
      "         [0.7137, 0.7176, 0.7216,  ..., 0.5294, 0.5255, 0.5216],\n",
      "         [0.7098, 0.7137, 0.7176,  ..., 0.5216, 0.5137, 0.5098]],\n",
      "\n",
      "        [[0.4824, 0.4824, 0.4863,  ..., 0.5686, 0.5569, 0.5529],\n",
      "         [0.4824, 0.4863, 0.4863,  ..., 0.5686, 0.5608, 0.5569],\n",
      "         [0.4863, 0.4863, 0.4863,  ..., 0.5686, 0.5608, 0.5569],\n",
      "         ...,\n",
      "         [0.7176, 0.7216, 0.7255,  ..., 0.5412, 0.5333, 0.5294],\n",
      "         [0.7137, 0.7176, 0.7216,  ..., 0.5294, 0.5255, 0.5216],\n",
      "         [0.7098, 0.7137, 0.7176,  ..., 0.5216, 0.5137, 0.5098]],\n",
      "\n",
      "        [[0.4824, 0.4824, 0.4863,  ..., 0.5686, 0.5569, 0.5529],\n",
      "         [0.4824, 0.4863, 0.4863,  ..., 0.5686, 0.5608, 0.5569],\n",
      "         [0.4863, 0.4863, 0.4863,  ..., 0.5686, 0.5608, 0.5569],\n",
      "         ...,\n",
      "         [0.7176, 0.7216, 0.7255,  ..., 0.5412, 0.5333, 0.5294],\n",
      "         [0.7137, 0.7176, 0.7216,  ..., 0.5294, 0.5255, 0.5216],\n",
      "         [0.7098, 0.7137, 0.7176,  ..., 0.5216, 0.5137, 0.5098]]]), tensor([79.0500, 65.2533]))\n"
     ]
    }
   ],
   "source": [
    "dataset_coords = PupilCoreDatasetCoords(\n",
    "    \"datasets/PupilCoreDataset/video5_eye0_video.avi\",\n",
    "    'datasets/PupilCoreDataset/video5_eye0_pupildata.csv',\n",
    "    \"datasets/PupilCoreDataset/video5_eye1_video.avi\",\n",
    "    'datasets/PupilCoreDataset/video5_eye1_pupildata.csv'\n",
    ")\n",
    "indices = torch.randperm(len(dataset_coords)).tolist()\n",
    "train_part = int(0.8 * len(dataset_coords))\n",
    "train_dataset = torch.utils.data.Subset(dataset_coords, indices[:train_part])\n",
    "test_dataset = torch.utils.data.Subset(dataset_coords, indices[train_part:])\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'test': len(test_dataset)\n",
    "}\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"test\": test_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_second_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    dataloaders,\n",
    "    dataset_sizes,\n",
    "):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            losses = []\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    # print(outputs, labels)\n",
    "                    outputs.to(device)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                losses.append(loss.item())\n",
    "                # print(running_loss)\n",
    "\n",
    "                if i % 40 == 0:\n",
    "                    print(f\"{phase}, Batch: {i}/{len(dataloaders[phase])} Loss: {np.mean(losses):.4f}\")\n",
    "            if phase == \"train\":\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            # epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            \n",
    "            # deep copy the model\n",
    "            # if phase == \"test\" and epoch_acc > best_acc:\n",
    "            #     best_acc = epoch_acc\n",
    "            #     best_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 218.4765\n",
      "train, Batch: 40/2390 Loss: 26.1347\n",
      "train, Batch: 80/2390 Loss: 20.9957\n",
      "train, Batch: 120/2390 Loss: 31.4391\n",
      "train, Batch: 160/2390 Loss: 36.6024\n",
      "train, Batch: 200/2390 Loss: 32.2342\n",
      "train, Batch: 240/2390 Loss: 33.8275\n",
      "train, Batch: 280/2390 Loss: 37.4270\n",
      "train, Batch: 320/2390 Loss: 36.7969\n",
      "train, Batch: 360/2390 Loss: 35.0424\n",
      "train, Batch: 400/2390 Loss: 36.3125\n",
      "train, Batch: 440/2390 Loss: 38.7166\n",
      "train, Batch: 480/2390 Loss: 37.7479\n",
      "train, Batch: 520/2390 Loss: 36.8760\n",
      "train, Batch: 560/2390 Loss: 36.6946\n",
      "train, Batch: 600/2390 Loss: 36.0792\n",
      "train, Batch: 640/2390 Loss: 39.1607\n",
      "train, Batch: 680/2390 Loss: 38.6209\n",
      "train, Batch: 720/2390 Loss: 40.1986\n",
      "train, Batch: 760/2390 Loss: 39.6427\n",
      "train, Batch: 800/2390 Loss: 42.1400\n",
      "train, Batch: 840/2390 Loss: 42.8787\n",
      "train, Batch: 880/2390 Loss: 41.3911\n",
      "train, Batch: 920/2390 Loss: 42.4264\n",
      "train, Batch: 960/2390 Loss: 40.9883\n",
      "train, Batch: 1000/2390 Loss: 40.7610\n",
      "train, Batch: 1040/2390 Loss: 41.9160\n",
      "train, Batch: 1080/2390 Loss: 42.1897\n",
      "train, Batch: 1120/2390 Loss: 44.6060\n",
      "train, Batch: 1160/2390 Loss: 43.9192\n",
      "train, Batch: 1200/2390 Loss: 43.6183\n",
      "train, Batch: 1240/2390 Loss: 42.6075\n",
      "train, Batch: 1280/2390 Loss: 41.7934\n",
      "train, Batch: 1320/2390 Loss: 41.4715\n",
      "train, Batch: 1360/2390 Loss: 40.3753\n",
      "train, Batch: 1400/2390 Loss: 39.2632\n",
      "train, Batch: 1440/2390 Loss: 39.5433\n",
      "train, Batch: 1480/2390 Loss: 39.5291\n",
      "train, Batch: 1520/2390 Loss: 38.8444\n",
      "train, Batch: 1560/2390 Loss: 39.4175\n",
      "train, Batch: 1600/2390 Loss: 38.9307\n",
      "train, Batch: 1640/2390 Loss: 39.0580\n",
      "train, Batch: 1680/2390 Loss: 38.2599\n",
      "train, Batch: 1720/2390 Loss: 39.5321\n",
      "train, Batch: 1760/2390 Loss: 39.1827\n",
      "train, Batch: 1800/2390 Loss: 39.7990\n",
      "train, Batch: 1840/2390 Loss: 39.8719\n",
      "train, Batch: 1880/2390 Loss: 41.0572\n",
      "train, Batch: 1920/2390 Loss: 40.7420\n",
      "train, Batch: 1960/2390 Loss: 40.2254\n",
      "train, Batch: 2000/2390 Loss: 39.8589\n",
      "train, Batch: 2040/2390 Loss: 40.0315\n",
      "train, Batch: 2080/2390 Loss: 41.1800\n",
      "train, Batch: 2120/2390 Loss: 41.4604\n",
      "train, Batch: 2160/2390 Loss: 40.9385\n",
      "train, Batch: 2200/2390 Loss: 40.5734\n",
      "train, Batch: 2240/2390 Loss: 40.4277\n",
      "train, Batch: 2280/2390 Loss: 40.1193\n",
      "train, Batch: 2320/2390 Loss: 39.5411\n",
      "train, Batch: 2360/2390 Loss: 38.9468\n",
      "test, Batch: 0/4779 Loss: 13.0703\n",
      "test, Batch: 40/4779 Loss: 9.6444\n",
      "test, Batch: 80/4779 Loss: 42.1478\n",
      "test, Batch: 120/4779 Loss: 31.5140\n",
      "test, Batch: 160/4779 Loss: 26.0213\n",
      "test, Batch: 200/4779 Loss: 22.1285\n",
      "test, Batch: 240/4779 Loss: 20.2033\n",
      "test, Batch: 280/4779 Loss: 18.2112\n",
      "test, Batch: 320/4779 Loss: 17.9831\n",
      "test, Batch: 360/4779 Loss: 16.5838\n",
      "test, Batch: 400/4779 Loss: 15.8407\n",
      "test, Batch: 440/4779 Loss: 28.5604\n",
      "test, Batch: 480/4779 Loss: 34.1117\n",
      "test, Batch: 520/4779 Loss: 31.9433\n",
      "test, Batch: 560/4779 Loss: 30.0194\n",
      "test, Batch: 600/4779 Loss: 28.7738\n",
      "test, Batch: 640/4779 Loss: 27.3221\n",
      "test, Batch: 680/4779 Loss: 44.3698\n",
      "test, Batch: 720/4779 Loss: 42.2015\n",
      "test, Batch: 760/4779 Loss: 40.2753\n",
      "test, Batch: 800/4779 Loss: 38.8264\n",
      "test, Batch: 840/4779 Loss: 37.2593\n",
      "test, Batch: 880/4779 Loss: 35.9889\n",
      "test, Batch: 920/4779 Loss: 34.9393\n",
      "test, Batch: 960/4779 Loss: 33.7778\n",
      "test, Batch: 1000/4779 Loss: 35.3247\n",
      "test, Batch: 1040/4779 Loss: 34.1866\n",
      "test, Batch: 1080/4779 Loss: 33.3646\n",
      "test, Batch: 1120/4779 Loss: 33.2556\n",
      "test, Batch: 1160/4779 Loss: 32.5249\n",
      "test, Batch: 1200/4779 Loss: 31.6680\n",
      "test, Batch: 1240/4779 Loss: 31.8457\n",
      "test, Batch: 1280/4779 Loss: 33.7314\n",
      "test, Batch: 1320/4779 Loss: 33.1050\n",
      "test, Batch: 1360/4779 Loss: 32.4763\n",
      "test, Batch: 1400/4779 Loss: 31.7654\n",
      "test, Batch: 1440/4779 Loss: 31.1976\n",
      "test, Batch: 1480/4779 Loss: 30.4887\n",
      "test, Batch: 1520/4779 Loss: 29.8498\n",
      "test, Batch: 1560/4779 Loss: 29.2371\n",
      "test, Batch: 1600/4779 Loss: 28.6690\n",
      "test, Batch: 1640/4779 Loss: 28.2495\n",
      "test, Batch: 1680/4779 Loss: 27.7303\n",
      "test, Batch: 1720/4779 Loss: 29.9756\n",
      "test, Batch: 1760/4779 Loss: 29.4455\n",
      "test, Batch: 1800/4779 Loss: 29.0466\n",
      "test, Batch: 1840/4779 Loss: 28.5257\n",
      "test, Batch: 1880/4779 Loss: 28.0330\n",
      "test, Batch: 1920/4779 Loss: 36.1667\n",
      "test, Batch: 1960/4779 Loss: 35.5489\n",
      "test, Batch: 2000/4779 Loss: 34.9713\n",
      "test, Batch: 2040/4779 Loss: 34.5093\n",
      "test, Batch: 2080/4779 Loss: 34.0580\n",
      "test, Batch: 2120/4779 Loss: 33.5325\n",
      "test, Batch: 2160/4779 Loss: 33.0239\n",
      "test, Batch: 2200/4779 Loss: 32.5876\n",
      "test, Batch: 2240/4779 Loss: 32.1716\n",
      "test, Batch: 2280/4779 Loss: 32.5741\n",
      "test, Batch: 2320/4779 Loss: 32.1227\n",
      "test, Batch: 2360/4779 Loss: 32.1907\n",
      "test, Batch: 2400/4779 Loss: 32.4030\n",
      "test, Batch: 2440/4779 Loss: 31.9465\n",
      "test, Batch: 2480/4779 Loss: 31.6061\n",
      "test, Batch: 2520/4779 Loss: 31.1944\n",
      "test, Batch: 2560/4779 Loss: 30.7909\n",
      "test, Batch: 2600/4779 Loss: 30.5335\n",
      "test, Batch: 2640/4779 Loss: 30.2904\n",
      "test, Batch: 2680/4779 Loss: 34.1971\n",
      "test, Batch: 2720/4779 Loss: 34.2102\n",
      "test, Batch: 2760/4779 Loss: 33.7864\n",
      "test, Batch: 2800/4779 Loss: 33.6135\n",
      "test, Batch: 2840/4779 Loss: 33.2660\n",
      "test, Batch: 2880/4779 Loss: 33.0725\n",
      "test, Batch: 2920/4779 Loss: 37.0231\n",
      "test, Batch: 2960/4779 Loss: 36.6604\n",
      "test, Batch: 3000/4779 Loss: 36.3055\n",
      "test, Batch: 3040/4779 Loss: 38.3925\n",
      "test, Batch: 3080/4779 Loss: 37.9710\n",
      "test, Batch: 3120/4779 Loss: 37.6230\n",
      "test, Batch: 3160/4779 Loss: 37.2206\n",
      "test, Batch: 3200/4779 Loss: 36.9746\n",
      "test, Batch: 3240/4779 Loss: 36.6043\n",
      "test, Batch: 3280/4779 Loss: 36.2377\n",
      "test, Batch: 3320/4779 Loss: 38.7279\n",
      "test, Batch: 3360/4779 Loss: 38.3908\n",
      "test, Batch: 3400/4779 Loss: 38.0103\n",
      "test, Batch: 3440/4779 Loss: 37.6298\n",
      "test, Batch: 3480/4779 Loss: 37.3224\n",
      "test, Batch: 3520/4779 Loss: 36.9623\n",
      "test, Batch: 3560/4779 Loss: 36.7379\n",
      "test, Batch: 3600/4779 Loss: 37.4428\n",
      "test, Batch: 3640/4779 Loss: 37.4930\n",
      "test, Batch: 3680/4779 Loss: 37.1539\n",
      "test, Batch: 3720/4779 Loss: 36.9292\n",
      "test, Batch: 3760/4779 Loss: 38.5439\n",
      "test, Batch: 3800/4779 Loss: 38.2069\n",
      "test, Batch: 3840/4779 Loss: 38.0955\n",
      "test, Batch: 3880/4779 Loss: 37.7571\n",
      "test, Batch: 3920/4779 Loss: 37.4583\n",
      "test, Batch: 3960/4779 Loss: 37.5799\n",
      "test, Batch: 4000/4779 Loss: 37.2627\n",
      "test, Batch: 4040/4779 Loss: 36.9461\n",
      "test, Batch: 4080/4779 Loss: 36.6912\n",
      "test, Batch: 4120/4779 Loss: 36.4391\n",
      "test, Batch: 4160/4779 Loss: 36.1628\n",
      "test, Batch: 4200/4779 Loss: 35.8635\n",
      "test, Batch: 4240/4779 Loss: 35.7204\n",
      "test, Batch: 4280/4779 Loss: 35.4675\n",
      "test, Batch: 4320/4779 Loss: 35.5210\n",
      "test, Batch: 4360/4779 Loss: 35.2462\n",
      "test, Batch: 4400/4779 Loss: 35.0454\n",
      "test, Batch: 4440/4779 Loss: 34.8141\n",
      "test, Batch: 4480/4779 Loss: 37.6049\n",
      "test, Batch: 4520/4779 Loss: 37.3224\n",
      "test, Batch: 4560/4779 Loss: 37.0435\n",
      "test, Batch: 4600/4779 Loss: 36.8752\n",
      "test, Batch: 4640/4779 Loss: 36.6062\n",
      "test, Batch: 4680/4779 Loss: 36.3390\n",
      "test, Batch: 4720/4779 Loss: 36.0852\n",
      "test, Batch: 4760/4779 Loss: 35.8302\n",
      "Epoch 1/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 6.2296\n",
      "train, Batch: 40/2390 Loss: 12.5048\n",
      "train, Batch: 80/2390 Loss: 37.3280\n",
      "train, Batch: 120/2390 Loss: 40.1373\n",
      "train, Batch: 160/2390 Loss: 45.8681\n",
      "train, Batch: 200/2390 Loss: 42.0711\n",
      "train, Batch: 240/2390 Loss: 39.1605\n",
      "train, Batch: 280/2390 Loss: 37.2852\n",
      "train, Batch: 320/2390 Loss: 39.6330\n",
      "train, Batch: 360/2390 Loss: 35.5940\n",
      "train, Batch: 400/2390 Loss: 34.6841\n",
      "train, Batch: 440/2390 Loss: 35.8850\n",
      "train, Batch: 480/2390 Loss: 34.2028\n",
      "train, Batch: 520/2390 Loss: 35.2557\n",
      "train, Batch: 560/2390 Loss: 33.6713\n",
      "train, Batch: 600/2390 Loss: 34.2612\n",
      "train, Batch: 640/2390 Loss: 32.5495\n",
      "train, Batch: 680/2390 Loss: 31.5245\n",
      "train, Batch: 720/2390 Loss: 31.4561\n",
      "train, Batch: 760/2390 Loss: 30.6228\n",
      "train, Batch: 800/2390 Loss: 31.3216\n",
      "train, Batch: 840/2390 Loss: 32.1680\n",
      "train, Batch: 880/2390 Loss: 30.9399\n",
      "train, Batch: 920/2390 Loss: 29.9084\n",
      "train, Batch: 960/2390 Loss: 28.9520\n",
      "train, Batch: 1000/2390 Loss: 32.8715\n",
      "train, Batch: 1040/2390 Loss: 32.0062\n",
      "train, Batch: 1080/2390 Loss: 30.9851\n",
      "train, Batch: 1120/2390 Loss: 30.1064\n",
      "train, Batch: 1160/2390 Loss: 30.6684\n",
      "train, Batch: 1200/2390 Loss: 30.5286\n",
      "train, Batch: 1240/2390 Loss: 31.3558\n",
      "train, Batch: 1280/2390 Loss: 30.7039\n",
      "train, Batch: 1320/2390 Loss: 29.9729\n",
      "train, Batch: 1360/2390 Loss: 29.8399\n",
      "train, Batch: 1400/2390 Loss: 31.5359\n",
      "train, Batch: 1440/2390 Loss: 31.3701\n",
      "train, Batch: 1480/2390 Loss: 30.6499\n",
      "train, Batch: 1520/2390 Loss: 30.5232\n",
      "train, Batch: 1560/2390 Loss: 30.3346\n",
      "train, Batch: 1600/2390 Loss: 30.4760\n",
      "train, Batch: 1640/2390 Loss: 30.6377\n",
      "train, Batch: 1680/2390 Loss: 30.1935\n",
      "train, Batch: 1720/2390 Loss: 30.4578\n",
      "train, Batch: 1760/2390 Loss: 31.8529\n",
      "train, Batch: 1800/2390 Loss: 32.9049\n",
      "train, Batch: 1840/2390 Loss: 32.9908\n",
      "train, Batch: 1880/2390 Loss: 34.0051\n",
      "train, Batch: 1920/2390 Loss: 34.6146\n",
      "train, Batch: 1960/2390 Loss: 34.1819\n",
      "train, Batch: 2000/2390 Loss: 34.0013\n",
      "train, Batch: 2040/2390 Loss: 35.2679\n",
      "train, Batch: 2080/2390 Loss: 34.7134\n",
      "train, Batch: 2120/2390 Loss: 34.3907\n",
      "train, Batch: 2160/2390 Loss: 34.5575\n",
      "train, Batch: 2200/2390 Loss: 34.1052\n",
      "train, Batch: 2240/2390 Loss: 34.4029\n",
      "train, Batch: 2280/2390 Loss: 34.0451\n",
      "train, Batch: 2320/2390 Loss: 34.8445\n",
      "train, Batch: 2360/2390 Loss: 34.3900\n",
      "test, Batch: 0/4779 Loss: 2.2066\n",
      "test, Batch: 40/4779 Loss: 8.8030\n",
      "test, Batch: 80/4779 Loss: 14.1300\n",
      "test, Batch: 120/4779 Loss: 26.8991\n",
      "test, Batch: 160/4779 Loss: 20.4788\n",
      "test, Batch: 200/4779 Loss: 18.7094\n",
      "test, Batch: 240/4779 Loss: 17.3636\n",
      "test, Batch: 280/4779 Loss: 16.4335\n",
      "test, Batch: 320/4779 Loss: 16.1299\n",
      "test, Batch: 360/4779 Loss: 14.4963\n",
      "test, Batch: 400/4779 Loss: 14.5948\n",
      "test, Batch: 440/4779 Loss: 20.4823\n",
      "test, Batch: 480/4779 Loss: 18.8749\n",
      "test, Batch: 520/4779 Loss: 37.4348\n",
      "test, Batch: 560/4779 Loss: 36.1345\n",
      "test, Batch: 600/4779 Loss: 39.9271\n",
      "test, Batch: 640/4779 Loss: 37.5492\n",
      "test, Batch: 680/4779 Loss: 47.9766\n",
      "test, Batch: 720/4779 Loss: 45.9933\n",
      "test, Batch: 760/4779 Loss: 43.6336\n",
      "test, Batch: 800/4779 Loss: 41.6321\n",
      "test, Batch: 840/4779 Loss: 39.7121\n",
      "test, Batch: 880/4779 Loss: 38.3964\n",
      "test, Batch: 920/4779 Loss: 41.0557\n",
      "test, Batch: 960/4779 Loss: 40.1453\n",
      "test, Batch: 1000/4779 Loss: 51.5074\n",
      "test, Batch: 1040/4779 Loss: 49.5733\n",
      "test, Batch: 1080/4779 Loss: 63.7262\n",
      "test, Batch: 1120/4779 Loss: 61.4927\n",
      "test, Batch: 1160/4779 Loss: 59.4535\n",
      "test, Batch: 1200/4779 Loss: 57.5236\n",
      "test, Batch: 1240/4779 Loss: 55.7096\n",
      "test, Batch: 1280/4779 Loss: 54.0080\n",
      "test, Batch: 1320/4779 Loss: 52.6784\n",
      "test, Batch: 1360/4779 Loss: 52.2023\n",
      "test, Batch: 1400/4779 Loss: 50.7445\n",
      "test, Batch: 1440/4779 Loss: 49.3754\n",
      "test, Batch: 1480/4779 Loss: 48.0749\n",
      "test, Batch: 1520/4779 Loss: 47.0438\n",
      "test, Batch: 1560/4779 Loss: 45.8645\n",
      "test, Batch: 1600/4779 Loss: 44.7412\n",
      "test, Batch: 1640/4779 Loss: 43.9514\n",
      "test, Batch: 1680/4779 Loss: 43.1142\n",
      "test, Batch: 1720/4779 Loss: 42.1364\n",
      "test, Batch: 1760/4779 Loss: 41.2105\n",
      "test, Batch: 1800/4779 Loss: 41.9656\n",
      "test, Batch: 1840/4779 Loss: 41.0926\n",
      "test, Batch: 1880/4779 Loss: 40.4081\n",
      "test, Batch: 1920/4779 Loss: 45.1504\n",
      "test, Batch: 1960/4779 Loss: 44.4007\n",
      "test, Batch: 2000/4779 Loss: 43.5417\n",
      "test, Batch: 2040/4779 Loss: 42.7240\n",
      "test, Batch: 2080/4779 Loss: 43.4004\n",
      "test, Batch: 2120/4779 Loss: 42.8348\n",
      "test, Batch: 2160/4779 Loss: 42.2113\n",
      "test, Batch: 2200/4779 Loss: 41.6971\n",
      "test, Batch: 2240/4779 Loss: 41.1393\n",
      "test, Batch: 2280/4779 Loss: 40.8225\n",
      "test, Batch: 2320/4779 Loss: 40.1379\n",
      "test, Batch: 2360/4779 Loss: 39.7818\n",
      "test, Batch: 2400/4779 Loss: 39.1416\n",
      "test, Batch: 2440/4779 Loss: 38.5218\n",
      "test, Batch: 2480/4779 Loss: 38.2091\n",
      "test, Batch: 2520/4779 Loss: 37.6242\n",
      "test, Batch: 2560/4779 Loss: 37.1100\n",
      "test, Batch: 2600/4779 Loss: 36.5660\n",
      "test, Batch: 2640/4779 Loss: 36.0341\n",
      "test, Batch: 2680/4779 Loss: 35.7091\n",
      "test, Batch: 2720/4779 Loss: 35.1984\n",
      "test, Batch: 2760/4779 Loss: 38.7708\n",
      "test, Batch: 2800/4779 Loss: 40.1042\n",
      "test, Batch: 2840/4779 Loss: 39.5537\n",
      "test, Batch: 2880/4779 Loss: 39.3221\n",
      "test, Batch: 2920/4779 Loss: 39.1278\n",
      "test, Batch: 2960/4779 Loss: 38.8391\n",
      "test, Batch: 3000/4779 Loss: 38.7224\n",
      "test, Batch: 3040/4779 Loss: 38.2290\n",
      "test, Batch: 3080/4779 Loss: 37.8913\n",
      "test, Batch: 3120/4779 Loss: 37.5612\n",
      "test, Batch: 3160/4779 Loss: 37.1616\n",
      "test, Batch: 3200/4779 Loss: 36.7089\n",
      "test, Batch: 3240/4779 Loss: 36.2690\n",
      "test, Batch: 3280/4779 Loss: 35.8417\n",
      "test, Batch: 3320/4779 Loss: 35.5472\n",
      "test, Batch: 3360/4779 Loss: 35.1373\n",
      "test, Batch: 3400/4779 Loss: 34.7388\n",
      "test, Batch: 3440/4779 Loss: 35.0091\n",
      "test, Batch: 3480/4779 Loss: 34.9037\n",
      "test, Batch: 3520/4779 Loss: 34.5962\n",
      "test, Batch: 3560/4779 Loss: 34.4729\n",
      "test, Batch: 3600/4779 Loss: 34.3645\n",
      "test, Batch: 3640/4779 Loss: 34.0843\n",
      "test, Batch: 3680/4779 Loss: 33.7269\n",
      "test, Batch: 3720/4779 Loss: 33.3779\n",
      "test, Batch: 3760/4779 Loss: 33.0371\n",
      "test, Batch: 3800/4779 Loss: 32.7721\n",
      "test, Batch: 3840/4779 Loss: 32.6637\n",
      "test, Batch: 3880/4779 Loss: 32.4878\n",
      "test, Batch: 3920/4779 Loss: 32.1686\n",
      "test, Batch: 3960/4779 Loss: 32.3411\n",
      "test, Batch: 4000/4779 Loss: 32.1324\n",
      "test, Batch: 4040/4779 Loss: 31.8285\n",
      "test, Batch: 4080/4779 Loss: 31.5263\n",
      "test, Batch: 4120/4779 Loss: 35.3829\n",
      "test, Batch: 4160/4779 Loss: 35.0696\n",
      "test, Batch: 4200/4779 Loss: 35.5404\n",
      "test, Batch: 4240/4779 Loss: 35.2155\n",
      "test, Batch: 4280/4779 Loss: 34.8980\n",
      "test, Batch: 4320/4779 Loss: 34.8429\n",
      "test, Batch: 4360/4779 Loss: 34.5320\n",
      "test, Batch: 4400/4779 Loss: 34.2367\n",
      "test, Batch: 4440/4779 Loss: 33.9600\n",
      "test, Batch: 4480/4779 Loss: 33.6677\n",
      "test, Batch: 4520/4779 Loss: 33.5432\n",
      "test, Batch: 4560/4779 Loss: 34.2657\n",
      "test, Batch: 4600/4779 Loss: 33.9802\n",
      "test, Batch: 4640/4779 Loss: 33.7848\n",
      "test, Batch: 4680/4779 Loss: 33.5942\n",
      "test, Batch: 4720/4779 Loss: 33.4063\n",
      "test, Batch: 4760/4779 Loss: 33.3995\n",
      "Epoch 2/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 1227.4867\n",
      "train, Batch: 40/2390 Loss: 44.6204\n",
      "train, Batch: 80/2390 Loss: 26.8549\n",
      "train, Batch: 120/2390 Loss: 31.0022\n",
      "train, Batch: 160/2390 Loss: 34.5374\n",
      "train, Batch: 200/2390 Loss: 31.1980\n",
      "train, Batch: 240/2390 Loss: 36.8478\n",
      "train, Batch: 280/2390 Loss: 32.5319\n",
      "train, Batch: 320/2390 Loss: 29.1847\n",
      "train, Batch: 360/2390 Loss: 33.1525\n",
      "train, Batch: 400/2390 Loss: 30.8412\n",
      "train, Batch: 440/2390 Loss: 33.7144\n",
      "train, Batch: 480/2390 Loss: 31.7404\n",
      "train, Batch: 520/2390 Loss: 30.0428\n",
      "train, Batch: 560/2390 Loss: 29.5265\n",
      "train, Batch: 600/2390 Loss: 28.7543\n",
      "train, Batch: 640/2390 Loss: 30.7845\n",
      "train, Batch: 680/2390 Loss: 31.8814\n",
      "train, Batch: 720/2390 Loss: 30.5219\n",
      "train, Batch: 760/2390 Loss: 33.3887\n",
      "train, Batch: 800/2390 Loss: 33.5598\n",
      "train, Batch: 840/2390 Loss: 34.0361\n",
      "train, Batch: 880/2390 Loss: 37.7570\n",
      "train, Batch: 920/2390 Loss: 38.0403\n",
      "train, Batch: 960/2390 Loss: 37.3596\n",
      "train, Batch: 1000/2390 Loss: 36.7389\n",
      "train, Batch: 1040/2390 Loss: 35.5417\n",
      "train, Batch: 1080/2390 Loss: 35.1505\n",
      "train, Batch: 1120/2390 Loss: 34.8986\n",
      "train, Batch: 1160/2390 Loss: 35.1555\n",
      "train, Batch: 1200/2390 Loss: 34.5221\n",
      "train, Batch: 1240/2390 Loss: 33.4596\n",
      "train, Batch: 1280/2390 Loss: 34.3238\n",
      "train, Batch: 1320/2390 Loss: 34.1808\n",
      "train, Batch: 1360/2390 Loss: 33.9948\n",
      "train, Batch: 1400/2390 Loss: 33.9346\n",
      "train, Batch: 1440/2390 Loss: 33.6824\n",
      "train, Batch: 1480/2390 Loss: 34.6113\n",
      "train, Batch: 1520/2390 Loss: 34.1547\n",
      "train, Batch: 1560/2390 Loss: 34.3972\n",
      "train, Batch: 1600/2390 Loss: 34.4214\n",
      "train, Batch: 1640/2390 Loss: 34.3301\n",
      "train, Batch: 1680/2390 Loss: 34.9903\n",
      "train, Batch: 1720/2390 Loss: 34.4637\n",
      "train, Batch: 1760/2390 Loss: 34.6681\n",
      "train, Batch: 1800/2390 Loss: 34.8983\n",
      "train, Batch: 1840/2390 Loss: 35.1572\n",
      "train, Batch: 1880/2390 Loss: 35.6518\n",
      "train, Batch: 1920/2390 Loss: 36.0221\n",
      "train, Batch: 1960/2390 Loss: 36.5284\n",
      "train, Batch: 2000/2390 Loss: 35.9871\n",
      "train, Batch: 2040/2390 Loss: 36.0832\n",
      "train, Batch: 2080/2390 Loss: 35.7399\n",
      "train, Batch: 2120/2390 Loss: 35.7530\n",
      "train, Batch: 2160/2390 Loss: 35.3771\n",
      "train, Batch: 2200/2390 Loss: 35.0071\n",
      "train, Batch: 2240/2390 Loss: 34.8828\n",
      "train, Batch: 2280/2390 Loss: 34.7352\n",
      "train, Batch: 2320/2390 Loss: 34.4398\n",
      "train, Batch: 2360/2390 Loss: 34.1017\n",
      "test, Batch: 0/4779 Loss: 2.6518\n",
      "test, Batch: 40/4779 Loss: 1.7569\n",
      "test, Batch: 80/4779 Loss: 3.3355\n",
      "test, Batch: 120/4779 Loss: 3.2870\n",
      "test, Batch: 160/4779 Loss: 2.9562\n",
      "test, Batch: 200/4779 Loss: 92.3758\n",
      "test, Batch: 240/4779 Loss: 77.3024\n",
      "test, Batch: 280/4779 Loss: 66.4816\n",
      "test, Batch: 320/4779 Loss: 58.3658\n",
      "test, Batch: 360/4779 Loss: 90.4711\n",
      "test, Batch: 400/4779 Loss: 81.6238\n",
      "test, Batch: 440/4779 Loss: 92.2520\n",
      "test, Batch: 480/4779 Loss: 87.0043\n",
      "test, Batch: 520/4779 Loss: 82.9941\n",
      "test, Batch: 560/4779 Loss: 77.1923\n",
      "test, Batch: 600/4779 Loss: 72.2635\n",
      "test, Batch: 640/4779 Loss: 72.8122\n",
      "test, Batch: 680/4779 Loss: 68.6343\n",
      "test, Batch: 720/4779 Loss: 65.7108\n",
      "test, Batch: 760/4779 Loss: 71.7059\n",
      "test, Batch: 800/4779 Loss: 68.2200\n",
      "test, Batch: 840/4779 Loss: 65.1447\n",
      "test, Batch: 880/4779 Loss: 62.3013\n",
      "test, Batch: 920/4779 Loss: 77.2643\n",
      "test, Batch: 960/4779 Loss: 85.2230\n",
      "test, Batch: 1000/4779 Loss: 82.0552\n",
      "test, Batch: 1040/4779 Loss: 78.9703\n",
      "test, Batch: 1080/4779 Loss: 84.8111\n",
      "test, Batch: 1120/4779 Loss: 81.8448\n",
      "test, Batch: 1160/4779 Loss: 80.3287\n",
      "test, Batch: 1200/4779 Loss: 77.7036\n",
      "test, Batch: 1240/4779 Loss: 77.9773\n",
      "test, Batch: 1280/4779 Loss: 75.6096\n",
      "test, Batch: 1320/4779 Loss: 73.7051\n",
      "test, Batch: 1360/4779 Loss: 75.0958\n",
      "test, Batch: 1400/4779 Loss: 73.0132\n",
      "test, Batch: 1440/4779 Loss: 74.3294\n",
      "test, Batch: 1480/4779 Loss: 72.7020\n",
      "test, Batch: 1520/4779 Loss: 70.8930\n",
      "test, Batch: 1560/4779 Loss: 69.1201\n",
      "test, Batch: 1600/4779 Loss: 67.4440\n",
      "test, Batch: 1640/4779 Loss: 65.9347\n",
      "test, Batch: 1680/4779 Loss: 64.4175\n",
      "test, Batch: 1720/4779 Loss: 62.9532\n",
      "test, Batch: 1760/4779 Loss: 61.5632\n",
      "test, Batch: 1800/4779 Loss: 60.4669\n",
      "test, Batch: 1840/4779 Loss: 59.1911\n",
      "test, Batch: 1880/4779 Loss: 57.9688\n",
      "test, Batch: 1920/4779 Loss: 56.8038\n",
      "test, Batch: 1960/4779 Loss: 55.6786\n",
      "test, Batch: 2000/4779 Loss: 54.6159\n",
      "test, Batch: 2040/4779 Loss: 53.6054\n",
      "test, Batch: 2080/4779 Loss: 52.7074\n",
      "test, Batch: 2120/4779 Loss: 51.7444\n",
      "test, Batch: 2160/4779 Loss: 50.8859\n",
      "test, Batch: 2200/4779 Loss: 49.9921\n",
      "test, Batch: 2240/4779 Loss: 49.1529\n",
      "test, Batch: 2280/4779 Loss: 48.3275\n",
      "test, Batch: 2320/4779 Loss: 47.5739\n",
      "test, Batch: 2360/4779 Loss: 46.7966\n",
      "test, Batch: 2400/4779 Loss: 46.0580\n",
      "test, Batch: 2440/4779 Loss: 45.3856\n",
      "test, Batch: 2480/4779 Loss: 50.8136\n",
      "test, Batch: 2520/4779 Loss: 50.0833\n",
      "test, Batch: 2560/4779 Loss: 49.3533\n",
      "test, Batch: 2600/4779 Loss: 48.6261\n",
      "test, Batch: 2640/4779 Loss: 47.9509\n",
      "test, Batch: 2680/4779 Loss: 47.5316\n",
      "test, Batch: 2720/4779 Loss: 48.5564\n",
      "test, Batch: 2760/4779 Loss: 47.9955\n",
      "test, Batch: 2800/4779 Loss: 47.3318\n",
      "test, Batch: 2840/4779 Loss: 46.6924\n",
      "test, Batch: 2880/4779 Loss: 46.0649\n",
      "test, Batch: 2920/4779 Loss: 45.5506\n",
      "test, Batch: 2960/4779 Loss: 44.9831\n",
      "test, Batch: 3000/4779 Loss: 44.4198\n",
      "test, Batch: 3040/4779 Loss: 43.8590\n",
      "test, Batch: 3080/4779 Loss: 43.7795\n",
      "test, Batch: 3120/4779 Loss: 43.2400\n",
      "test, Batch: 3160/4779 Loss: 42.7171\n",
      "test, Batch: 3200/4779 Loss: 42.2089\n",
      "test, Batch: 3240/4779 Loss: 41.7881\n",
      "test, Batch: 3280/4779 Loss: 41.3152\n",
      "test, Batch: 3320/4779 Loss: 40.8510\n",
      "test, Batch: 3360/4779 Loss: 40.4200\n",
      "test, Batch: 3400/4779 Loss: 39.9634\n",
      "test, Batch: 3440/4779 Loss: 39.5325\n",
      "test, Batch: 3480/4779 Loss: 43.0682\n",
      "test, Batch: 3520/4779 Loss: 42.6036\n",
      "test, Batch: 3560/4779 Loss: 42.1473\n",
      "test, Batch: 3600/4779 Loss: 41.6999\n",
      "test, Batch: 3640/4779 Loss: 41.2646\n",
      "test, Batch: 3680/4779 Loss: 40.8321\n",
      "test, Batch: 3720/4779 Loss: 40.5674\n",
      "test, Batch: 3760/4779 Loss: 40.1517\n",
      "test, Batch: 3800/4779 Loss: 39.7636\n",
      "test, Batch: 3840/4779 Loss: 39.3898\n",
      "test, Batch: 3880/4779 Loss: 39.0225\n",
      "test, Batch: 3920/4779 Loss: 39.0194\n",
      "test, Batch: 3960/4779 Loss: 38.6403\n",
      "test, Batch: 4000/4779 Loss: 38.2730\n",
      "test, Batch: 4040/4779 Loss: 38.3337\n",
      "test, Batch: 4080/4779 Loss: 37.9749\n",
      "test, Batch: 4120/4779 Loss: 37.6808\n",
      "test, Batch: 4160/4779 Loss: 37.3363\n",
      "test, Batch: 4200/4779 Loss: 37.0056\n",
      "test, Batch: 4240/4779 Loss: 36.6788\n",
      "test, Batch: 4280/4779 Loss: 36.3669\n",
      "test, Batch: 4320/4779 Loss: 36.0844\n",
      "test, Batch: 4360/4779 Loss: 35.7672\n",
      "test, Batch: 4400/4779 Loss: 35.7125\n",
      "test, Batch: 4440/4779 Loss: 35.4234\n",
      "test, Batch: 4480/4779 Loss: 35.1340\n",
      "test, Batch: 4520/4779 Loss: 34.8337\n",
      "test, Batch: 4560/4779 Loss: 34.7390\n",
      "test, Batch: 4600/4779 Loss: 34.8274\n",
      "test, Batch: 4640/4779 Loss: 34.6161\n",
      "test, Batch: 4680/4779 Loss: 34.4120\n",
      "test, Batch: 4720/4779 Loss: 34.1818\n",
      "test, Batch: 4760/4779 Loss: 33.9234\n",
      "Epoch 3/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.5028\n",
      "train, Batch: 40/2390 Loss: 10.6547\n",
      "train, Batch: 80/2390 Loss: 22.1141\n",
      "train, Batch: 120/2390 Loss: 15.4715\n",
      "train, Batch: 160/2390 Loss: 12.2628\n",
      "train, Batch: 200/2390 Loss: 21.5843\n",
      "train, Batch: 240/2390 Loss: 30.0765\n",
      "train, Batch: 280/2390 Loss: 26.4586\n",
      "train, Batch: 320/2390 Loss: 27.7602\n",
      "train, Batch: 360/2390 Loss: 33.1778\n",
      "train, Batch: 400/2390 Loss: 30.9500\n",
      "train, Batch: 440/2390 Loss: 33.6986\n",
      "train, Batch: 480/2390 Loss: 34.4001\n",
      "train, Batch: 520/2390 Loss: 34.8119\n",
      "train, Batch: 560/2390 Loss: 33.8234\n",
      "train, Batch: 600/2390 Loss: 32.3248\n",
      "train, Batch: 640/2390 Loss: 36.3188\n",
      "train, Batch: 680/2390 Loss: 35.6125\n",
      "train, Batch: 720/2390 Loss: 34.3459\n",
      "train, Batch: 760/2390 Loss: 34.7688\n",
      "train, Batch: 800/2390 Loss: 34.1572\n",
      "train, Batch: 840/2390 Loss: 33.3231\n",
      "train, Batch: 880/2390 Loss: 32.7561\n",
      "train, Batch: 920/2390 Loss: 31.5642\n",
      "train, Batch: 960/2390 Loss: 32.9855\n",
      "train, Batch: 1000/2390 Loss: 31.8805\n",
      "train, Batch: 1040/2390 Loss: 32.5283\n",
      "train, Batch: 1080/2390 Loss: 34.8449\n",
      "train, Batch: 1120/2390 Loss: 33.9106\n",
      "train, Batch: 1160/2390 Loss: 33.1256\n",
      "train, Batch: 1200/2390 Loss: 32.1944\n",
      "train, Batch: 1240/2390 Loss: 31.3105\n",
      "train, Batch: 1280/2390 Loss: 30.4917\n",
      "train, Batch: 1320/2390 Loss: 30.2541\n",
      "train, Batch: 1360/2390 Loss: 30.0398\n",
      "train, Batch: 1400/2390 Loss: 29.6281\n",
      "train, Batch: 1440/2390 Loss: 30.3940\n",
      "train, Batch: 1480/2390 Loss: 29.7432\n",
      "train, Batch: 1520/2390 Loss: 30.1909\n",
      "train, Batch: 1560/2390 Loss: 30.8985\n",
      "train, Batch: 1600/2390 Loss: 31.1918\n",
      "train, Batch: 1640/2390 Loss: 30.6918\n",
      "train, Batch: 1680/2390 Loss: 31.1731\n",
      "train, Batch: 1720/2390 Loss: 31.5457\n",
      "train, Batch: 1760/2390 Loss: 32.0601\n",
      "train, Batch: 1800/2390 Loss: 31.4474\n",
      "train, Batch: 1840/2390 Loss: 32.0163\n",
      "train, Batch: 1880/2390 Loss: 31.4293\n",
      "train, Batch: 1920/2390 Loss: 31.7392\n",
      "train, Batch: 1960/2390 Loss: 33.4225\n",
      "train, Batch: 2000/2390 Loss: 33.0917\n",
      "train, Batch: 2040/2390 Loss: 34.3290\n",
      "train, Batch: 2080/2390 Loss: 34.2071\n",
      "train, Batch: 2120/2390 Loss: 33.7255\n",
      "train, Batch: 2160/2390 Loss: 33.6848\n",
      "train, Batch: 2200/2390 Loss: 33.2126\n",
      "train, Batch: 2240/2390 Loss: 33.5810\n",
      "train, Batch: 2280/2390 Loss: 33.1686\n",
      "train, Batch: 2320/2390 Loss: 32.6363\n",
      "train, Batch: 2360/2390 Loss: 32.4432\n",
      "test, Batch: 0/4779 Loss: 0.1505\n",
      "test, Batch: 40/4779 Loss: 0.4897\n",
      "test, Batch: 80/4779 Loss: 0.4899\n",
      "test, Batch: 120/4779 Loss: 13.4949\n",
      "test, Batch: 160/4779 Loss: 10.2770\n",
      "test, Batch: 200/4779 Loss: 46.7681\n",
      "test, Batch: 240/4779 Loss: 40.6131\n",
      "test, Batch: 280/4779 Loss: 34.9119\n",
      "test, Batch: 320/4779 Loss: 31.6351\n",
      "test, Batch: 360/4779 Loss: 28.1809\n",
      "test, Batch: 400/4779 Loss: 38.2206\n",
      "test, Batch: 440/4779 Loss: 34.7830\n",
      "test, Batch: 480/4779 Loss: 32.8607\n",
      "test, Batch: 520/4779 Loss: 30.3816\n",
      "test, Batch: 560/4779 Loss: 28.4303\n",
      "test, Batch: 600/4779 Loss: 26.7541\n",
      "test, Batch: 640/4779 Loss: 25.3583\n",
      "test, Batch: 680/4779 Loss: 23.9469\n",
      "test, Batch: 720/4779 Loss: 22.6571\n",
      "test, Batch: 760/4779 Loss: 21.5218\n",
      "test, Batch: 800/4779 Loss: 20.7578\n",
      "test, Batch: 840/4779 Loss: 19.7897\n",
      "test, Batch: 880/4779 Loss: 18.9134\n",
      "test, Batch: 920/4779 Loss: 33.9234\n",
      "test, Batch: 960/4779 Loss: 32.6646\n",
      "test, Batch: 1000/4779 Loss: 42.3568\n",
      "test, Batch: 1040/4779 Loss: 41.1316\n",
      "test, Batch: 1080/4779 Loss: 39.6359\n",
      "test, Batch: 1120/4779 Loss: 38.2850\n",
      "test, Batch: 1160/4779 Loss: 37.3808\n",
      "test, Batch: 1200/4779 Loss: 36.2805\n",
      "test, Batch: 1240/4779 Loss: 38.3263\n",
      "test, Batch: 1280/4779 Loss: 37.2746\n",
      "test, Batch: 1320/4779 Loss: 36.1607\n",
      "test, Batch: 1360/4779 Loss: 35.2028\n",
      "test, Batch: 1400/4779 Loss: 34.2081\n",
      "test, Batch: 1440/4779 Loss: 33.2740\n",
      "test, Batch: 1480/4779 Loss: 32.3876\n",
      "test, Batch: 1520/4779 Loss: 38.3496\n",
      "test, Batch: 1560/4779 Loss: 37.3968\n",
      "test, Batch: 1600/4779 Loss: 36.9131\n",
      "test, Batch: 1640/4779 Loss: 39.0055\n",
      "test, Batch: 1680/4779 Loss: 38.0897\n",
      "test, Batch: 1720/4779 Loss: 37.2833\n",
      "test, Batch: 1760/4779 Loss: 36.5330\n",
      "test, Batch: 1800/4779 Loss: 35.8700\n",
      "test, Batch: 1840/4779 Loss: 35.1055\n",
      "test, Batch: 1880/4779 Loss: 34.3728\n",
      "test, Batch: 1920/4779 Loss: 33.8582\n",
      "test, Batch: 1960/4779 Loss: 33.2686\n",
      "test, Batch: 2000/4779 Loss: 32.6124\n",
      "test, Batch: 2040/4779 Loss: 32.0377\n",
      "test, Batch: 2080/4779 Loss: 31.4299\n",
      "test, Batch: 2120/4779 Loss: 30.8453\n",
      "test, Batch: 2160/4779 Loss: 30.3571\n",
      "test, Batch: 2200/4779 Loss: 30.0048\n",
      "test, Batch: 2240/4779 Loss: 29.4808\n",
      "test, Batch: 2280/4779 Loss: 28.9725\n",
      "test, Batch: 2320/4779 Loss: 28.4827\n",
      "test, Batch: 2360/4779 Loss: 34.9933\n",
      "test, Batch: 2400/4779 Loss: 34.4208\n",
      "test, Batch: 2440/4779 Loss: 34.0527\n",
      "test, Batch: 2480/4779 Loss: 33.5120\n",
      "test, Batch: 2520/4779 Loss: 33.0398\n",
      "test, Batch: 2560/4779 Loss: 32.7895\n",
      "test, Batch: 2600/4779 Loss: 32.2948\n",
      "test, Batch: 2640/4779 Loss: 36.6960\n",
      "test, Batch: 2680/4779 Loss: 36.2047\n",
      "test, Batch: 2720/4779 Loss: 35.6820\n",
      "test, Batch: 2760/4779 Loss: 39.2582\n",
      "test, Batch: 2800/4779 Loss: 39.1010\n",
      "test, Batch: 2840/4779 Loss: 38.6652\n",
      "test, Batch: 2880/4779 Loss: 38.1375\n",
      "test, Batch: 2920/4779 Loss: 37.6224\n",
      "test, Batch: 2960/4779 Loss: 37.1216\n",
      "test, Batch: 3000/4779 Loss: 36.7275\n",
      "test, Batch: 3040/4779 Loss: 37.4302\n",
      "test, Batch: 3080/4779 Loss: 37.1070\n",
      "test, Batch: 3120/4779 Loss: 38.8521\n",
      "test, Batch: 3160/4779 Loss: 38.8002\n",
      "test, Batch: 3200/4779 Loss: 38.3787\n",
      "test, Batch: 3240/4779 Loss: 38.0177\n",
      "test, Batch: 3280/4779 Loss: 37.5608\n",
      "test, Batch: 3320/4779 Loss: 37.1147\n",
      "test, Batch: 3360/4779 Loss: 37.2601\n",
      "test, Batch: 3400/4779 Loss: 37.6426\n",
      "test, Batch: 3440/4779 Loss: 37.2127\n",
      "test, Batch: 3480/4779 Loss: 36.9894\n",
      "test, Batch: 3520/4779 Loss: 36.5773\n",
      "test, Batch: 3560/4779 Loss: 36.3015\n",
      "test, Batch: 3600/4779 Loss: 35.9031\n",
      "test, Batch: 3640/4779 Loss: 40.9796\n",
      "test, Batch: 3680/4779 Loss: 40.6681\n",
      "test, Batch: 3720/4779 Loss: 41.4758\n",
      "test, Batch: 3760/4779 Loss: 41.4603\n",
      "test, Batch: 3800/4779 Loss: 41.8039\n",
      "test, Batch: 3840/4779 Loss: 41.3734\n",
      "test, Batch: 3880/4779 Loss: 41.0138\n",
      "test, Batch: 3920/4779 Loss: 40.6006\n",
      "test, Batch: 3960/4779 Loss: 40.1947\n",
      "test, Batch: 4000/4779 Loss: 39.8552\n",
      "test, Batch: 4040/4779 Loss: 39.4660\n",
      "test, Batch: 4080/4779 Loss: 41.7432\n",
      "test, Batch: 4120/4779 Loss: 41.3431\n",
      "test, Batch: 4160/4779 Loss: 41.1038\n",
      "test, Batch: 4200/4779 Loss: 40.7173\n",
      "test, Batch: 4240/4779 Loss: 40.7138\n",
      "test, Batch: 4280/4779 Loss: 40.3427\n",
      "test, Batch: 4320/4779 Loss: 39.9752\n",
      "test, Batch: 4360/4779 Loss: 39.6128\n",
      "test, Batch: 4400/4779 Loss: 39.7209\n",
      "test, Batch: 4440/4779 Loss: 39.3676\n",
      "test, Batch: 4480/4779 Loss: 39.0223\n",
      "test, Batch: 4520/4779 Loss: 39.7369\n",
      "test, Batch: 4560/4779 Loss: 39.6147\n",
      "test, Batch: 4600/4779 Loss: 39.2995\n",
      "test, Batch: 4640/4779 Loss: 38.9654\n",
      "test, Batch: 4680/4779 Loss: 38.6374\n",
      "test, Batch: 4720/4779 Loss: 38.3145\n",
      "test, Batch: 4760/4779 Loss: 37.9966\n",
      "Epoch 4/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 1119.9044\n",
      "train, Batch: 40/2390 Loss: 86.0116\n",
      "train, Batch: 80/2390 Loss: 82.0890\n",
      "train, Batch: 120/2390 Loss: 62.6905\n",
      "train, Batch: 160/2390 Loss: 69.5560\n",
      "train, Batch: 200/2390 Loss: 57.3029\n",
      "train, Batch: 240/2390 Loss: 49.9406\n",
      "train, Batch: 280/2390 Loss: 45.7685\n",
      "train, Batch: 320/2390 Loss: 43.7120\n",
      "train, Batch: 360/2390 Loss: 44.4521\n",
      "train, Batch: 400/2390 Loss: 40.2640\n",
      "train, Batch: 440/2390 Loss: 39.4308\n",
      "train, Batch: 480/2390 Loss: 44.8858\n",
      "train, Batch: 520/2390 Loss: 42.4898\n",
      "train, Batch: 560/2390 Loss: 40.4568\n",
      "train, Batch: 600/2390 Loss: 39.5267\n",
      "train, Batch: 640/2390 Loss: 38.2523\n",
      "train, Batch: 680/2390 Loss: 36.7263\n",
      "train, Batch: 720/2390 Loss: 38.9558\n",
      "train, Batch: 760/2390 Loss: 40.0595\n",
      "train, Batch: 800/2390 Loss: 38.3569\n",
      "train, Batch: 840/2390 Loss: 36.6550\n",
      "train, Batch: 880/2390 Loss: 36.4357\n",
      "train, Batch: 920/2390 Loss: 35.0872\n",
      "train, Batch: 960/2390 Loss: 34.1958\n",
      "train, Batch: 1000/2390 Loss: 32.9220\n",
      "train, Batch: 1040/2390 Loss: 33.1373\n",
      "train, Batch: 1080/2390 Loss: 31.9800\n",
      "train, Batch: 1120/2390 Loss: 30.9741\n",
      "train, Batch: 1160/2390 Loss: 29.9252\n",
      "train, Batch: 1200/2390 Loss: 30.6947\n",
      "train, Batch: 1240/2390 Loss: 30.1399\n",
      "train, Batch: 1280/2390 Loss: 29.3977\n",
      "train, Batch: 1320/2390 Loss: 29.1566\n",
      "train, Batch: 1360/2390 Loss: 28.8265\n",
      "train, Batch: 1400/2390 Loss: 28.4962\n",
      "train, Batch: 1440/2390 Loss: 27.8237\n",
      "train, Batch: 1480/2390 Loss: 27.3878\n",
      "train, Batch: 1520/2390 Loss: 27.5159\n",
      "train, Batch: 1560/2390 Loss: 27.4042\n",
      "train, Batch: 1600/2390 Loss: 28.7979\n",
      "train, Batch: 1640/2390 Loss: 28.6814\n",
      "train, Batch: 1680/2390 Loss: 30.0657\n",
      "train, Batch: 1720/2390 Loss: 29.7971\n",
      "train, Batch: 1760/2390 Loss: 29.1745\n",
      "train, Batch: 1800/2390 Loss: 29.1975\n",
      "train, Batch: 1840/2390 Loss: 29.2032\n",
      "train, Batch: 1880/2390 Loss: 29.4472\n",
      "train, Batch: 1920/2390 Loss: 29.1047\n",
      "train, Batch: 1960/2390 Loss: 29.8850\n",
      "train, Batch: 2000/2390 Loss: 30.2710\n",
      "train, Batch: 2040/2390 Loss: 30.7576\n",
      "train, Batch: 2080/2390 Loss: 30.4231\n",
      "train, Batch: 2120/2390 Loss: 30.0522\n",
      "train, Batch: 2160/2390 Loss: 30.6827\n",
      "train, Batch: 2200/2390 Loss: 30.5088\n",
      "train, Batch: 2240/2390 Loss: 30.1351\n",
      "train, Batch: 2280/2390 Loss: 30.9430\n",
      "train, Batch: 2320/2390 Loss: 30.6451\n",
      "train, Batch: 2360/2390 Loss: 31.1885\n",
      "test, Batch: 0/4779 Loss: 1.7808\n",
      "test, Batch: 40/4779 Loss: 2.2674\n",
      "test, Batch: 80/4779 Loss: 2.4786\n",
      "test, Batch: 120/4779 Loss: 4.2574\n",
      "test, Batch: 160/4779 Loss: 3.9565\n",
      "test, Batch: 200/4779 Loss: 3.6860\n",
      "test, Batch: 240/4779 Loss: 3.9583\n",
      "test, Batch: 280/4779 Loss: 3.7288\n",
      "test, Batch: 320/4779 Loss: 4.2690\n",
      "test, Batch: 360/4779 Loss: 7.2505\n",
      "test, Batch: 400/4779 Loss: 6.7838\n",
      "test, Batch: 440/4779 Loss: 6.4126\n",
      "test, Batch: 480/4779 Loss: 6.5064\n",
      "test, Batch: 520/4779 Loss: 6.2470\n",
      "test, Batch: 560/4779 Loss: 27.2845\n",
      "test, Batch: 600/4779 Loss: 25.6229\n",
      "test, Batch: 640/4779 Loss: 24.4109\n",
      "test, Batch: 680/4779 Loss: 24.1910\n",
      "test, Batch: 720/4779 Loss: 22.9856\n",
      "test, Batch: 760/4779 Loss: 22.0748\n",
      "test, Batch: 800/4779 Loss: 21.0937\n",
      "test, Batch: 840/4779 Loss: 20.2353\n",
      "test, Batch: 880/4779 Loss: 19.9362\n",
      "test, Batch: 920/4779 Loss: 36.4250\n",
      "test, Batch: 960/4779 Loss: 35.4462\n",
      "test, Batch: 1000/4779 Loss: 34.1208\n",
      "test, Batch: 1040/4779 Loss: 32.8962\n",
      "test, Batch: 1080/4779 Loss: 31.7856\n",
      "test, Batch: 1120/4779 Loss: 30.9646\n",
      "test, Batch: 1160/4779 Loss: 29.9988\n",
      "test, Batch: 1200/4779 Loss: 29.5097\n",
      "test, Batch: 1240/4779 Loss: 28.6423\n",
      "test, Batch: 1280/4779 Loss: 28.3335\n",
      "test, Batch: 1320/4779 Loss: 31.7014\n",
      "test, Batch: 1360/4779 Loss: 31.0999\n",
      "test, Batch: 1400/4779 Loss: 30.2869\n",
      "test, Batch: 1440/4779 Loss: 29.5148\n",
      "test, Batch: 1480/4779 Loss: 29.7067\n",
      "test, Batch: 1520/4779 Loss: 28.9802\n",
      "test, Batch: 1560/4779 Loss: 28.8553\n",
      "test, Batch: 1600/4779 Loss: 31.9952\n",
      "test, Batch: 1640/4779 Loss: 31.4619\n",
      "test, Batch: 1680/4779 Loss: 30.7703\n",
      "test, Batch: 1720/4779 Loss: 30.1469\n",
      "test, Batch: 1760/4779 Loss: 29.5073\n",
      "test, Batch: 1800/4779 Loss: 31.3996\n",
      "test, Batch: 1840/4779 Loss: 30.7803\n",
      "test, Batch: 1880/4779 Loss: 32.6934\n",
      "test, Batch: 1920/4779 Loss: 32.0668\n",
      "test, Batch: 1960/4779 Loss: 31.4645\n",
      "test, Batch: 2000/4779 Loss: 31.1082\n",
      "test, Batch: 2040/4779 Loss: 30.6821\n",
      "test, Batch: 2080/4779 Loss: 30.1356\n",
      "test, Batch: 2120/4779 Loss: 29.6140\n",
      "test, Batch: 2160/4779 Loss: 29.3914\n",
      "test, Batch: 2200/4779 Loss: 28.9623\n",
      "test, Batch: 2240/4779 Loss: 28.8346\n",
      "test, Batch: 2280/4779 Loss: 28.3658\n",
      "test, Batch: 2320/4779 Loss: 27.9103\n",
      "test, Batch: 2360/4779 Loss: 27.5320\n",
      "test, Batch: 2400/4779 Loss: 27.1092\n",
      "test, Batch: 2440/4779 Loss: 26.7073\n",
      "test, Batch: 2480/4779 Loss: 26.9419\n",
      "test, Batch: 2520/4779 Loss: 26.6365\n",
      "test, Batch: 2560/4779 Loss: 26.2557\n",
      "test, Batch: 2600/4779 Loss: 32.1941\n",
      "test, Batch: 2640/4779 Loss: 31.7447\n",
      "test, Batch: 2680/4779 Loss: 31.3789\n",
      "test, Batch: 2720/4779 Loss: 30.9598\n",
      "test, Batch: 2760/4779 Loss: 30.5482\n",
      "test, Batch: 2800/4779 Loss: 32.3231\n",
      "test, Batch: 2840/4779 Loss: 31.9031\n",
      "test, Batch: 2880/4779 Loss: 31.5402\n",
      "test, Batch: 2920/4779 Loss: 31.2457\n",
      "test, Batch: 2960/4779 Loss: 31.3997\n",
      "test, Batch: 3000/4779 Loss: 31.0115\n",
      "test, Batch: 3040/4779 Loss: 30.7614\n",
      "test, Batch: 3080/4779 Loss: 30.4000\n",
      "test, Batch: 3120/4779 Loss: 30.0450\n",
      "test, Batch: 3160/4779 Loss: 30.0557\n",
      "test, Batch: 3200/4779 Loss: 32.8757\n",
      "test, Batch: 3240/4779 Loss: 32.5738\n",
      "test, Batch: 3280/4779 Loss: 32.2108\n",
      "test, Batch: 3320/4779 Loss: 31.8523\n",
      "test, Batch: 3360/4779 Loss: 31.5003\n",
      "test, Batch: 3400/4779 Loss: 31.2015\n",
      "test, Batch: 3440/4779 Loss: 34.9553\n",
      "test, Batch: 3480/4779 Loss: 34.6352\n",
      "test, Batch: 3520/4779 Loss: 34.3171\n",
      "test, Batch: 3560/4779 Loss: 36.2260\n",
      "test, Batch: 3600/4779 Loss: 35.8456\n",
      "test, Batch: 3640/4779 Loss: 35.4744\n",
      "test, Batch: 3680/4779 Loss: 35.3461\n",
      "test, Batch: 3720/4779 Loss: 34.9895\n",
      "test, Batch: 3760/4779 Loss: 34.6927\n",
      "test, Batch: 3800/4779 Loss: 34.3540\n",
      "test, Batch: 3840/4779 Loss: 38.0936\n",
      "test, Batch: 3880/4779 Loss: 42.3656\n",
      "test, Batch: 3920/4779 Loss: 41.9560\n",
      "test, Batch: 3960/4779 Loss: 41.5613\n",
      "test, Batch: 4000/4779 Loss: 41.1655\n",
      "test, Batch: 4040/4779 Loss: 40.7827\n",
      "test, Batch: 4080/4779 Loss: 40.4071\n",
      "test, Batch: 4120/4779 Loss: 40.0921\n",
      "test, Batch: 4160/4779 Loss: 39.7296\n",
      "test, Batch: 4200/4779 Loss: 39.4131\n",
      "test, Batch: 4240/4779 Loss: 39.1112\n",
      "test, Batch: 4280/4779 Loss: 38.8055\n",
      "test, Batch: 4320/4779 Loss: 38.5166\n",
      "test, Batch: 4360/4779 Loss: 38.1811\n",
      "test, Batch: 4400/4779 Loss: 38.0083\n",
      "test, Batch: 4440/4779 Loss: 37.7736\n",
      "test, Batch: 4480/4779 Loss: 37.9491\n",
      "test, Batch: 4520/4779 Loss: 37.6609\n",
      "test, Batch: 4560/4779 Loss: 39.1956\n",
      "test, Batch: 4600/4779 Loss: 38.9213\n",
      "test, Batch: 4640/4779 Loss: 38.6507\n",
      "test, Batch: 4680/4779 Loss: 38.3779\n",
      "test, Batch: 4720/4779 Loss: 38.1664\n",
      "test, Batch: 4760/4779 Loss: 37.8668\n",
      "Epoch 5/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 3.2546\n",
      "train, Batch: 40/2390 Loss: 27.4422\n",
      "train, Batch: 80/2390 Loss: 32.2207\n",
      "train, Batch: 120/2390 Loss: 23.0100\n",
      "train, Batch: 160/2390 Loss: 22.1796\n",
      "train, Batch: 200/2390 Loss: 25.7284\n",
      "train, Batch: 240/2390 Loss: 23.0494\n",
      "train, Batch: 280/2390 Loss: 27.4955\n",
      "train, Batch: 320/2390 Loss: 30.9628\n",
      "train, Batch: 360/2390 Loss: 30.4398\n",
      "train, Batch: 400/2390 Loss: 28.5903\n",
      "train, Batch: 440/2390 Loss: 34.5667\n",
      "train, Batch: 480/2390 Loss: 32.2887\n",
      "train, Batch: 520/2390 Loss: 30.2031\n",
      "train, Batch: 560/2390 Loss: 28.4313\n",
      "train, Batch: 600/2390 Loss: 29.0650\n",
      "train, Batch: 640/2390 Loss: 28.2024\n",
      "train, Batch: 680/2390 Loss: 30.5278\n",
      "train, Batch: 720/2390 Loss: 29.3381\n",
      "train, Batch: 760/2390 Loss: 33.2155\n",
      "train, Batch: 800/2390 Loss: 32.5093\n",
      "train, Batch: 840/2390 Loss: 31.8691\n",
      "train, Batch: 880/2390 Loss: 31.4541\n",
      "train, Batch: 920/2390 Loss: 31.3379\n",
      "train, Batch: 960/2390 Loss: 35.0251\n",
      "train, Batch: 1000/2390 Loss: 34.5173\n",
      "train, Batch: 1040/2390 Loss: 34.8677\n",
      "train, Batch: 1080/2390 Loss: 33.7032\n",
      "train, Batch: 1120/2390 Loss: 32.7141\n",
      "train, Batch: 1160/2390 Loss: 34.7592\n",
      "train, Batch: 1200/2390 Loss: 35.1334\n",
      "train, Batch: 1240/2390 Loss: 34.1867\n",
      "train, Batch: 1280/2390 Loss: 33.3262\n",
      "train, Batch: 1320/2390 Loss: 33.6733\n",
      "train, Batch: 1360/2390 Loss: 33.4276\n",
      "train, Batch: 1400/2390 Loss: 33.6572\n",
      "train, Batch: 1440/2390 Loss: 32.9338\n",
      "train, Batch: 1480/2390 Loss: 32.6079\n",
      "train, Batch: 1520/2390 Loss: 31.9011\n",
      "train, Batch: 1560/2390 Loss: 33.8433\n",
      "train, Batch: 1600/2390 Loss: 33.8656\n",
      "train, Batch: 1640/2390 Loss: 33.8854\n",
      "train, Batch: 1680/2390 Loss: 34.8683\n",
      "train, Batch: 1720/2390 Loss: 35.4126\n",
      "train, Batch: 1760/2390 Loss: 34.7260\n",
      "train, Batch: 1800/2390 Loss: 34.1925\n",
      "train, Batch: 1840/2390 Loss: 33.4930\n",
      "train, Batch: 1880/2390 Loss: 33.4011\n",
      "train, Batch: 1920/2390 Loss: 34.3053\n",
      "train, Batch: 1960/2390 Loss: 33.7352\n",
      "train, Batch: 2000/2390 Loss: 33.6117\n",
      "train, Batch: 2040/2390 Loss: 33.0717\n",
      "train, Batch: 2080/2390 Loss: 32.8293\n",
      "train, Batch: 2120/2390 Loss: 33.1380\n",
      "train, Batch: 2160/2390 Loss: 33.2437\n",
      "train, Batch: 2200/2390 Loss: 32.7301\n",
      "train, Batch: 2240/2390 Loss: 33.7212\n",
      "train, Batch: 2280/2390 Loss: 33.4950\n",
      "train, Batch: 2320/2390 Loss: 33.6066\n",
      "train, Batch: 2360/2390 Loss: 33.0714\n",
      "test, Batch: 0/4779 Loss: 0.4333\n",
      "test, Batch: 40/4779 Loss: 0.5936\n",
      "test, Batch: 80/4779 Loss: 20.9620\n",
      "test, Batch: 120/4779 Loss: 45.4483\n",
      "test, Batch: 160/4779 Loss: 46.2494\n",
      "test, Batch: 200/4779 Loss: 43.0032\n",
      "test, Batch: 240/4779 Loss: 40.5693\n",
      "test, Batch: 280/4779 Loss: 35.6901\n",
      "test, Batch: 320/4779 Loss: 32.0251\n",
      "test, Batch: 360/4779 Loss: 28.5442\n",
      "test, Batch: 400/4779 Loss: 26.3082\n",
      "test, Batch: 440/4779 Loss: 23.9794\n",
      "test, Batch: 480/4779 Loss: 22.0357\n",
      "test, Batch: 520/4779 Loss: 20.6834\n",
      "test, Batch: 560/4779 Loss: 19.2552\n",
      "test, Batch: 600/4779 Loss: 39.0435\n",
      "test, Batch: 640/4779 Loss: 36.6529\n",
      "test, Batch: 680/4779 Loss: 34.5319\n",
      "test, Batch: 720/4779 Loss: 33.1984\n",
      "test, Batch: 760/4779 Loss: 31.7320\n",
      "test, Batch: 800/4779 Loss: 49.4845\n",
      "test, Batch: 840/4779 Loss: 47.1975\n",
      "test, Batch: 880/4779 Loss: 45.1485\n",
      "test, Batch: 920/4779 Loss: 43.2106\n",
      "test, Batch: 960/4779 Loss: 41.4332\n",
      "test, Batch: 1000/4779 Loss: 39.8041\n",
      "test, Batch: 1040/4779 Loss: 38.2951\n",
      "test, Batch: 1080/4779 Loss: 37.1037\n",
      "test, Batch: 1120/4779 Loss: 36.0815\n",
      "test, Batch: 1160/4779 Loss: 34.8579\n",
      "test, Batch: 1200/4779 Loss: 35.1553\n",
      "test, Batch: 1240/4779 Loss: 34.0506\n",
      "test, Batch: 1280/4779 Loss: 33.0088\n",
      "test, Batch: 1320/4779 Loss: 32.0273\n",
      "test, Batch: 1360/4779 Loss: 31.5632\n",
      "test, Batch: 1400/4779 Loss: 30.6774\n",
      "test, Batch: 1440/4779 Loss: 29.8407\n",
      "test, Batch: 1480/4779 Loss: 29.0538\n",
      "test, Batch: 1520/4779 Loss: 28.3086\n",
      "test, Batch: 1560/4779 Loss: 27.8827\n",
      "test, Batch: 1600/4779 Loss: 27.3022\n",
      "test, Batch: 1640/4779 Loss: 26.7185\n",
      "test, Batch: 1680/4779 Loss: 27.0210\n",
      "test, Batch: 1720/4779 Loss: 28.4733\n",
      "test, Batch: 1760/4779 Loss: 27.9279\n",
      "test, Batch: 1800/4779 Loss: 27.3241\n",
      "test, Batch: 1840/4779 Loss: 26.7439\n",
      "test, Batch: 1880/4779 Loss: 26.1933\n",
      "test, Batch: 1920/4779 Loss: 25.7943\n",
      "test, Batch: 1960/4779 Loss: 25.2976\n",
      "test, Batch: 2000/4779 Loss: 24.8037\n",
      "test, Batch: 2040/4779 Loss: 24.5394\n",
      "test, Batch: 2080/4779 Loss: 24.0802\n",
      "test, Batch: 2120/4779 Loss: 25.0038\n",
      "test, Batch: 2160/4779 Loss: 24.5636\n",
      "test, Batch: 2200/4779 Loss: 24.1446\n",
      "test, Batch: 2240/4779 Loss: 23.7954\n",
      "test, Batch: 2280/4779 Loss: 26.0265\n",
      "test, Batch: 2320/4779 Loss: 25.8114\n",
      "test, Batch: 2360/4779 Loss: 25.5886\n",
      "test, Batch: 2400/4779 Loss: 25.2358\n",
      "test, Batch: 2440/4779 Loss: 26.1801\n",
      "test, Batch: 2480/4779 Loss: 25.7684\n",
      "test, Batch: 2520/4779 Loss: 28.4939\n",
      "test, Batch: 2560/4779 Loss: 28.1319\n",
      "test, Batch: 2600/4779 Loss: 28.2638\n",
      "test, Batch: 2640/4779 Loss: 27.9519\n",
      "test, Batch: 2680/4779 Loss: 27.5431\n",
      "test, Batch: 2720/4779 Loss: 28.9720\n",
      "test, Batch: 2760/4779 Loss: 32.2751\n",
      "test, Batch: 2800/4779 Loss: 36.7538\n",
      "test, Batch: 2840/4779 Loss: 36.3305\n",
      "test, Batch: 2880/4779 Loss: 35.8359\n",
      "test, Batch: 2920/4779 Loss: 35.3526\n",
      "test, Batch: 2960/4779 Loss: 36.4692\n",
      "test, Batch: 3000/4779 Loss: 36.0664\n",
      "test, Batch: 3040/4779 Loss: 35.6061\n",
      "test, Batch: 3080/4779 Loss: 35.1515\n",
      "test, Batch: 3120/4779 Loss: 34.7090\n",
      "test, Batch: 3160/4779 Loss: 34.3350\n",
      "test, Batch: 3200/4779 Loss: 33.9901\n",
      "test, Batch: 3240/4779 Loss: 33.5796\n",
      "test, Batch: 3280/4779 Loss: 33.2235\n",
      "test, Batch: 3320/4779 Loss: 33.1745\n",
      "test, Batch: 3360/4779 Loss: 32.8348\n",
      "test, Batch: 3400/4779 Loss: 32.4545\n",
      "test, Batch: 3440/4779 Loss: 32.1311\n",
      "test, Batch: 3480/4779 Loss: 31.8300\n",
      "test, Batch: 3520/4779 Loss: 31.4752\n",
      "test, Batch: 3560/4779 Loss: 31.2014\n",
      "test, Batch: 3600/4779 Loss: 30.8609\n",
      "test, Batch: 3640/4779 Loss: 30.5332\n",
      "test, Batch: 3680/4779 Loss: 30.2688\n",
      "test, Batch: 3720/4779 Loss: 29.9515\n",
      "test, Batch: 3760/4779 Loss: 29.8029\n",
      "test, Batch: 3800/4779 Loss: 32.4511\n",
      "test, Batch: 3840/4779 Loss: 32.1200\n",
      "test, Batch: 3880/4779 Loss: 31.7956\n",
      "test, Batch: 3920/4779 Loss: 34.4918\n",
      "test, Batch: 3960/4779 Loss: 34.1938\n",
      "test, Batch: 4000/4779 Loss: 34.0273\n",
      "test, Batch: 4040/4779 Loss: 36.4610\n",
      "test, Batch: 4080/4779 Loss: 36.2131\n",
      "test, Batch: 4120/4779 Loss: 35.8681\n",
      "test, Batch: 4160/4779 Loss: 35.5304\n",
      "test, Batch: 4200/4779 Loss: 35.2538\n",
      "test, Batch: 4240/4779 Loss: 35.2083\n",
      "test, Batch: 4280/4779 Loss: 34.9217\n",
      "test, Batch: 4320/4779 Loss: 34.6599\n",
      "test, Batch: 4360/4779 Loss: 34.3536\n",
      "test, Batch: 4400/4779 Loss: 34.0465\n",
      "test, Batch: 4440/4779 Loss: 33.8770\n",
      "test, Batch: 4480/4779 Loss: 33.7445\n",
      "test, Batch: 4520/4779 Loss: 33.4505\n",
      "test, Batch: 4560/4779 Loss: 33.1615\n",
      "test, Batch: 4600/4779 Loss: 32.9172\n",
      "test, Batch: 4640/4779 Loss: 33.8942\n",
      "test, Batch: 4680/4779 Loss: 33.6562\n",
      "test, Batch: 4720/4779 Loss: 33.3762\n",
      "test, Batch: 4760/4779 Loss: 33.3494\n",
      "Epoch 6/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.4632\n",
      "train, Batch: 40/2390 Loss: 44.8640\n",
      "train, Batch: 80/2390 Loss: 36.9868\n",
      "train, Batch: 120/2390 Loss: 31.9255\n",
      "train, Batch: 160/2390 Loss: 37.3644\n",
      "train, Batch: 200/2390 Loss: 30.8869\n",
      "train, Batch: 240/2390 Loss: 26.1775\n",
      "train, Batch: 280/2390 Loss: 25.8268\n",
      "train, Batch: 320/2390 Loss: 29.3736\n",
      "train, Batch: 360/2390 Loss: 26.6341\n",
      "train, Batch: 400/2390 Loss: 30.1400\n",
      "train, Batch: 440/2390 Loss: 28.6470\n",
      "train, Batch: 480/2390 Loss: 26.6754\n",
      "train, Batch: 520/2390 Loss: 31.9485\n",
      "train, Batch: 560/2390 Loss: 32.0538\n",
      "train, Batch: 600/2390 Loss: 31.1218\n",
      "train, Batch: 640/2390 Loss: 29.7323\n",
      "train, Batch: 680/2390 Loss: 29.7732\n",
      "train, Batch: 720/2390 Loss: 28.3965\n",
      "train, Batch: 760/2390 Loss: 27.4711\n",
      "train, Batch: 800/2390 Loss: 26.6333\n",
      "train, Batch: 840/2390 Loss: 27.1445\n",
      "train, Batch: 880/2390 Loss: 26.5883\n",
      "train, Batch: 920/2390 Loss: 25.6709\n",
      "train, Batch: 960/2390 Loss: 24.7389\n",
      "train, Batch: 1000/2390 Loss: 25.7639\n",
      "train, Batch: 1040/2390 Loss: 25.8956\n",
      "train, Batch: 1080/2390 Loss: 25.5473\n",
      "train, Batch: 1120/2390 Loss: 25.9726\n",
      "train, Batch: 1160/2390 Loss: 27.4808\n",
      "train, Batch: 1200/2390 Loss: 27.7674\n",
      "train, Batch: 1240/2390 Loss: 27.3384\n",
      "train, Batch: 1280/2390 Loss: 26.6495\n",
      "train, Batch: 1320/2390 Loss: 26.3008\n",
      "train, Batch: 1360/2390 Loss: 27.8871\n",
      "train, Batch: 1400/2390 Loss: 27.4853\n",
      "train, Batch: 1440/2390 Loss: 28.1202\n",
      "train, Batch: 1480/2390 Loss: 28.1632\n",
      "train, Batch: 1520/2390 Loss: 28.3668\n",
      "train, Batch: 1560/2390 Loss: 28.9011\n",
      "train, Batch: 1600/2390 Loss: 29.6771\n",
      "train, Batch: 1640/2390 Loss: 29.1475\n",
      "train, Batch: 1680/2390 Loss: 29.1951\n",
      "train, Batch: 1720/2390 Loss: 28.7381\n",
      "train, Batch: 1760/2390 Loss: 29.0683\n",
      "train, Batch: 1800/2390 Loss: 28.5084\n",
      "train, Batch: 1840/2390 Loss: 28.0344\n",
      "train, Batch: 1880/2390 Loss: 28.9833\n",
      "train, Batch: 1920/2390 Loss: 29.0440\n",
      "train, Batch: 1960/2390 Loss: 29.0922\n",
      "train, Batch: 2000/2390 Loss: 29.4989\n",
      "train, Batch: 2040/2390 Loss: 30.0889\n",
      "train, Batch: 2080/2390 Loss: 30.4550\n",
      "train, Batch: 2120/2390 Loss: 30.6730\n",
      "train, Batch: 2160/2390 Loss: 31.8061\n",
      "train, Batch: 2200/2390 Loss: 31.4602\n",
      "train, Batch: 2240/2390 Loss: 31.2773\n",
      "train, Batch: 2280/2390 Loss: 31.1857\n",
      "train, Batch: 2320/2390 Loss: 31.1766\n",
      "train, Batch: 2360/2390 Loss: 30.8045\n",
      "test, Batch: 0/4779 Loss: 1.0471\n",
      "test, Batch: 40/4779 Loss: 347.9516\n",
      "test, Batch: 80/4779 Loss: 313.4597\n",
      "test, Batch: 120/4779 Loss: 214.4609\n",
      "test, Batch: 160/4779 Loss: 163.1502\n",
      "test, Batch: 200/4779 Loss: 131.3140\n",
      "test, Batch: 240/4779 Loss: 109.8952\n",
      "test, Batch: 280/4779 Loss: 94.4548\n",
      "test, Batch: 320/4779 Loss: 82.8159\n",
      "test, Batch: 360/4779 Loss: 74.4919\n",
      "test, Batch: 400/4779 Loss: 68.5607\n",
      "test, Batch: 440/4779 Loss: 70.6437\n",
      "test, Batch: 480/4779 Loss: 64.9602\n",
      "test, Batch: 520/4779 Loss: 73.4412\n",
      "test, Batch: 560/4779 Loss: 68.2817\n",
      "test, Batch: 600/4779 Loss: 84.8172\n",
      "test, Batch: 640/4779 Loss: 94.8194\n",
      "test, Batch: 680/4779 Loss: 89.3086\n",
      "test, Batch: 720/4779 Loss: 85.3329\n",
      "test, Batch: 760/4779 Loss: 80.9418\n",
      "test, Batch: 800/4779 Loss: 76.9783\n",
      "test, Batch: 840/4779 Loss: 73.3715\n",
      "test, Batch: 880/4779 Loss: 70.5993\n",
      "test, Batch: 920/4779 Loss: 68.0297\n",
      "test, Batch: 960/4779 Loss: 65.2771\n",
      "test, Batch: 1000/4779 Loss: 62.9169\n",
      "test, Batch: 1040/4779 Loss: 60.5528\n",
      "test, Batch: 1080/4779 Loss: 58.7087\n",
      "test, Batch: 1120/4779 Loss: 57.3196\n",
      "test, Batch: 1160/4779 Loss: 55.4282\n",
      "test, Batch: 1200/4779 Loss: 53.7252\n",
      "test, Batch: 1240/4779 Loss: 52.0353\n",
      "test, Batch: 1280/4779 Loss: 50.4832\n",
      "test, Batch: 1320/4779 Loss: 48.9855\n",
      "test, Batch: 1360/4779 Loss: 52.4085\n",
      "test, Batch: 1400/4779 Loss: 57.8859\n",
      "test, Batch: 1440/4779 Loss: 56.5103\n",
      "test, Batch: 1480/4779 Loss: 55.2354\n",
      "test, Batch: 1520/4779 Loss: 54.3448\n",
      "test, Batch: 1560/4779 Loss: 52.9825\n",
      "test, Batch: 1600/4779 Loss: 52.0380\n",
      "test, Batch: 1640/4779 Loss: 51.0061\n",
      "test, Batch: 1680/4779 Loss: 49.9394\n",
      "test, Batch: 1720/4779 Loss: 48.8192\n",
      "test, Batch: 1760/4779 Loss: 47.8400\n",
      "test, Batch: 1800/4779 Loss: 46.9123\n",
      "test, Batch: 1840/4779 Loss: 47.1510\n",
      "test, Batch: 1880/4779 Loss: 46.1914\n",
      "test, Batch: 1920/4779 Loss: 45.2961\n",
      "test, Batch: 1960/4779 Loss: 44.4361\n",
      "test, Batch: 2000/4779 Loss: 43.8234\n",
      "test, Batch: 2040/4779 Loss: 43.0987\n",
      "test, Batch: 2080/4779 Loss: 42.6973\n",
      "test, Batch: 2120/4779 Loss: 44.0236\n",
      "test, Batch: 2160/4779 Loss: 43.2318\n",
      "test, Batch: 2200/4779 Loss: 42.6585\n",
      "test, Batch: 2240/4779 Loss: 41.9730\n",
      "test, Batch: 2280/4779 Loss: 47.8523\n",
      "test, Batch: 2320/4779 Loss: 47.0456\n",
      "test, Batch: 2360/4779 Loss: 46.2667\n",
      "test, Batch: 2400/4779 Loss: 45.6180\n",
      "test, Batch: 2440/4779 Loss: 45.2514\n",
      "test, Batch: 2480/4779 Loss: 44.6794\n",
      "test, Batch: 2520/4779 Loss: 43.9929\n",
      "test, Batch: 2560/4779 Loss: 43.3320\n",
      "test, Batch: 2600/4779 Loss: 42.6877\n",
      "test, Batch: 2640/4779 Loss: 42.0609\n",
      "test, Batch: 2680/4779 Loss: 41.4496\n",
      "test, Batch: 2720/4779 Loss: 45.1464\n",
      "test, Batch: 2760/4779 Loss: 44.5390\n",
      "test, Batch: 2800/4779 Loss: 44.3511\n",
      "test, Batch: 2840/4779 Loss: 43.7458\n",
      "test, Batch: 2880/4779 Loss: 43.1540\n",
      "test, Batch: 2920/4779 Loss: 42.8067\n",
      "test, Batch: 2960/4779 Loss: 42.4607\n",
      "test, Batch: 3000/4779 Loss: 41.9098\n",
      "test, Batch: 3040/4779 Loss: 42.7026\n",
      "test, Batch: 3080/4779 Loss: 42.2145\n",
      "test, Batch: 3120/4779 Loss: 41.6910\n",
      "test, Batch: 3160/4779 Loss: 41.2462\n",
      "test, Batch: 3200/4779 Loss: 40.7482\n",
      "test, Batch: 3240/4779 Loss: 40.2829\n",
      "test, Batch: 3280/4779 Loss: 40.3561\n",
      "test, Batch: 3320/4779 Loss: 39.8830\n",
      "test, Batch: 3360/4779 Loss: 39.7174\n",
      "test, Batch: 3400/4779 Loss: 39.2692\n",
      "test, Batch: 3440/4779 Loss: 39.0179\n",
      "test, Batch: 3480/4779 Loss: 39.2637\n",
      "test, Batch: 3520/4779 Loss: 38.8746\n",
      "test, Batch: 3560/4779 Loss: 38.5404\n",
      "test, Batch: 3600/4779 Loss: 38.8005\n",
      "test, Batch: 3640/4779 Loss: 38.4245\n",
      "test, Batch: 3680/4779 Loss: 38.0197\n",
      "test, Batch: 3720/4779 Loss: 37.6238\n",
      "test, Batch: 3760/4779 Loss: 37.2468\n",
      "test, Batch: 3800/4779 Loss: 36.8639\n",
      "test, Batch: 3840/4779 Loss: 37.1135\n",
      "test, Batch: 3880/4779 Loss: 36.7543\n",
      "test, Batch: 3920/4779 Loss: 36.4322\n",
      "test, Batch: 3960/4779 Loss: 36.0786\n",
      "test, Batch: 4000/4779 Loss: 35.7294\n",
      "test, Batch: 4040/4779 Loss: 35.8224\n",
      "test, Batch: 4080/4779 Loss: 35.4832\n",
      "test, Batch: 4120/4779 Loss: 35.7629\n",
      "test, Batch: 4160/4779 Loss: 35.4286\n",
      "test, Batch: 4200/4779 Loss: 35.1040\n",
      "test, Batch: 4240/4779 Loss: 35.3856\n",
      "test, Batch: 4280/4779 Loss: 35.1075\n",
      "test, Batch: 4320/4779 Loss: 34.7935\n",
      "test, Batch: 4360/4779 Loss: 34.5715\n",
      "test, Batch: 4400/4779 Loss: 34.2846\n",
      "test, Batch: 4440/4779 Loss: 33.9863\n",
      "test, Batch: 4480/4779 Loss: 33.6999\n",
      "test, Batch: 4520/4779 Loss: 33.6271\n",
      "test, Batch: 4560/4779 Loss: 33.3427\n",
      "test, Batch: 4600/4779 Loss: 33.0634\n",
      "test, Batch: 4640/4779 Loss: 32.9686\n",
      "test, Batch: 4680/4779 Loss: 32.9640\n",
      "test, Batch: 4720/4779 Loss: 32.8547\n",
      "test, Batch: 4760/4779 Loss: 32.5948\n",
      "Epoch 7/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 1.0558\n",
      "train, Batch: 40/2390 Loss: 47.2562\n",
      "train, Batch: 80/2390 Loss: 32.8391\n",
      "train, Batch: 120/2390 Loss: 24.5008\n",
      "train, Batch: 160/2390 Loss: 28.0521\n",
      "train, Batch: 200/2390 Loss: 34.0618\n",
      "train, Batch: 240/2390 Loss: 37.6090\n",
      "train, Batch: 280/2390 Loss: 35.3815\n",
      "train, Batch: 320/2390 Loss: 31.3123\n",
      "train, Batch: 360/2390 Loss: 28.0043\n",
      "train, Batch: 400/2390 Loss: 25.3104\n",
      "train, Batch: 440/2390 Loss: 23.1124\n",
      "train, Batch: 480/2390 Loss: 22.2424\n",
      "train, Batch: 520/2390 Loss: 31.2007\n",
      "train, Batch: 560/2390 Loss: 30.7526\n",
      "train, Batch: 600/2390 Loss: 29.6386\n",
      "train, Batch: 640/2390 Loss: 29.3481\n",
      "train, Batch: 680/2390 Loss: 28.3127\n",
      "train, Batch: 720/2390 Loss: 28.2967\n",
      "train, Batch: 760/2390 Loss: 29.6683\n",
      "train, Batch: 800/2390 Loss: 31.2645\n",
      "train, Batch: 840/2390 Loss: 30.1084\n",
      "train, Batch: 880/2390 Loss: 30.1666\n",
      "train, Batch: 920/2390 Loss: 29.0516\n",
      "train, Batch: 960/2390 Loss: 27.9952\n",
      "train, Batch: 1000/2390 Loss: 27.2840\n",
      "train, Batch: 1040/2390 Loss: 26.8722\n",
      "train, Batch: 1080/2390 Loss: 28.0069\n",
      "train, Batch: 1120/2390 Loss: 27.2259\n",
      "train, Batch: 1160/2390 Loss: 26.9252\n",
      "train, Batch: 1200/2390 Loss: 29.4926\n",
      "train, Batch: 1240/2390 Loss: 28.6274\n",
      "train, Batch: 1280/2390 Loss: 28.0143\n",
      "train, Batch: 1320/2390 Loss: 27.3791\n",
      "train, Batch: 1360/2390 Loss: 27.1830\n",
      "train, Batch: 1400/2390 Loss: 27.4264\n",
      "train, Batch: 1440/2390 Loss: 26.8674\n",
      "train, Batch: 1480/2390 Loss: 26.9775\n",
      "train, Batch: 1520/2390 Loss: 27.4329\n",
      "train, Batch: 1560/2390 Loss: 26.9372\n",
      "train, Batch: 1600/2390 Loss: 27.2348\n",
      "train, Batch: 1640/2390 Loss: 26.7722\n",
      "train, Batch: 1680/2390 Loss: 26.2393\n",
      "train, Batch: 1720/2390 Loss: 26.1897\n",
      "train, Batch: 1760/2390 Loss: 26.6213\n",
      "train, Batch: 1800/2390 Loss: 26.6967\n",
      "train, Batch: 1840/2390 Loss: 26.3388\n",
      "train, Batch: 1880/2390 Loss: 26.0527\n",
      "train, Batch: 1920/2390 Loss: 25.6360\n",
      "train, Batch: 1960/2390 Loss: 25.5128\n",
      "train, Batch: 2000/2390 Loss: 26.2096\n",
      "train, Batch: 2040/2390 Loss: 26.7640\n",
      "train, Batch: 2080/2390 Loss: 26.7837\n",
      "train, Batch: 2120/2390 Loss: 26.4774\n",
      "train, Batch: 2160/2390 Loss: 26.1678\n",
      "train, Batch: 2200/2390 Loss: 26.0804\n",
      "train, Batch: 2240/2390 Loss: 25.7293\n",
      "train, Batch: 2280/2390 Loss: 26.8091\n",
      "train, Batch: 2320/2390 Loss: 27.3364\n",
      "train, Batch: 2360/2390 Loss: 27.2015\n",
      "test, Batch: 0/4779 Loss: 0.0987\n",
      "test, Batch: 40/4779 Loss: 61.0129\n",
      "test, Batch: 80/4779 Loss: 34.3793\n",
      "test, Batch: 120/4779 Loss: 23.0898\n",
      "test, Batch: 160/4779 Loss: 17.3947\n",
      "test, Batch: 200/4779 Loss: 14.0876\n",
      "test, Batch: 240/4779 Loss: 12.4314\n",
      "test, Batch: 280/4779 Loss: 11.6436\n",
      "test, Batch: 320/4779 Loss: 12.0032\n",
      "test, Batch: 360/4779 Loss: 12.3833\n",
      "test, Batch: 400/4779 Loss: 11.1628\n",
      "test, Batch: 440/4779 Loss: 10.1676\n",
      "test, Batch: 480/4779 Loss: 10.3023\n",
      "test, Batch: 520/4779 Loss: 33.6198\n",
      "test, Batch: 560/4779 Loss: 31.2384\n",
      "test, Batch: 600/4779 Loss: 29.2005\n",
      "test, Batch: 640/4779 Loss: 27.7844\n",
      "test, Batch: 680/4779 Loss: 26.1643\n",
      "test, Batch: 720/4779 Loss: 24.7217\n",
      "test, Batch: 760/4779 Loss: 24.1265\n",
      "test, Batch: 800/4779 Loss: 23.1405\n",
      "test, Batch: 840/4779 Loss: 27.9386\n",
      "test, Batch: 880/4779 Loss: 26.6779\n",
      "test, Batch: 920/4779 Loss: 25.5301\n",
      "test, Batch: 960/4779 Loss: 24.4737\n",
      "test, Batch: 1000/4779 Loss: 23.5111\n",
      "test, Batch: 1040/4779 Loss: 22.6194\n",
      "test, Batch: 1080/4779 Loss: 21.9722\n",
      "test, Batch: 1120/4779 Loss: 25.0666\n",
      "test, Batch: 1160/4779 Loss: 24.2076\n",
      "test, Batch: 1200/4779 Loss: 23.5272\n",
      "test, Batch: 1240/4779 Loss: 22.8072\n",
      "test, Batch: 1280/4779 Loss: 22.1027\n",
      "test, Batch: 1320/4779 Loss: 21.4395\n",
      "test, Batch: 1360/4779 Loss: 20.8137\n",
      "test, Batch: 1400/4779 Loss: 20.2490\n",
      "test, Batch: 1440/4779 Loss: 19.7778\n",
      "test, Batch: 1480/4779 Loss: 21.1951\n",
      "test, Batch: 1520/4779 Loss: 20.6450\n",
      "test, Batch: 1560/4779 Loss: 20.1194\n",
      "test, Batch: 1600/4779 Loss: 21.8014\n",
      "test, Batch: 1640/4779 Loss: 22.1414\n",
      "test, Batch: 1680/4779 Loss: 21.6184\n",
      "test, Batch: 1720/4779 Loss: 27.4858\n",
      "test, Batch: 1760/4779 Loss: 26.8682\n",
      "test, Batch: 1800/4779 Loss: 26.2757\n",
      "test, Batch: 1840/4779 Loss: 25.8352\n",
      "test, Batch: 1880/4779 Loss: 32.7807\n",
      "test, Batch: 1920/4779 Loss: 32.1020\n",
      "test, Batch: 1960/4779 Loss: 31.5419\n",
      "test, Batch: 2000/4779 Loss: 30.9147\n",
      "test, Batch: 2040/4779 Loss: 30.4293\n",
      "test, Batch: 2080/4779 Loss: 29.9759\n",
      "test, Batch: 2120/4779 Loss: 29.4141\n",
      "test, Batch: 2160/4779 Loss: 28.9413\n",
      "test, Batch: 2200/4779 Loss: 28.4180\n",
      "test, Batch: 2240/4779 Loss: 28.0817\n",
      "test, Batch: 2280/4779 Loss: 27.5913\n",
      "test, Batch: 2320/4779 Loss: 27.1181\n",
      "test, Batch: 2360/4779 Loss: 26.7158\n",
      "test, Batch: 2400/4779 Loss: 26.6208\n",
      "test, Batch: 2440/4779 Loss: 26.5645\n",
      "test, Batch: 2480/4779 Loss: 26.1383\n",
      "test, Batch: 2520/4779 Loss: 25.9283\n",
      "test, Batch: 2560/4779 Loss: 27.0610\n",
      "test, Batch: 2600/4779 Loss: 26.6478\n",
      "test, Batch: 2640/4779 Loss: 26.2470\n",
      "test, Batch: 2680/4779 Loss: 25.8645\n",
      "test, Batch: 2720/4779 Loss: 25.4860\n",
      "test, Batch: 2760/4779 Loss: 25.1262\n",
      "test, Batch: 2800/4779 Loss: 25.0815\n",
      "test, Batch: 2840/4779 Loss: 24.7310\n",
      "test, Batch: 2880/4779 Loss: 24.5184\n",
      "test, Batch: 2920/4779 Loss: 26.3000\n",
      "test, Batch: 2960/4779 Loss: 25.9475\n",
      "test, Batch: 3000/4779 Loss: 25.7258\n",
      "test, Batch: 3040/4779 Loss: 25.4407\n",
      "test, Batch: 3080/4779 Loss: 31.2040\n",
      "test, Batch: 3120/4779 Loss: 30.9602\n",
      "test, Batch: 3160/4779 Loss: 30.5701\n",
      "test, Batch: 3200/4779 Loss: 30.1917\n",
      "test, Batch: 3240/4779 Loss: 30.0717\n",
      "test, Batch: 3280/4779 Loss: 29.7980\n",
      "test, Batch: 3320/4779 Loss: 30.7146\n",
      "test, Batch: 3360/4779 Loss: 30.3511\n",
      "test, Batch: 3400/4779 Loss: 34.7768\n",
      "test, Batch: 3440/4779 Loss: 34.4117\n",
      "test, Batch: 3480/4779 Loss: 34.3666\n",
      "test, Batch: 3520/4779 Loss: 34.0479\n",
      "test, Batch: 3560/4779 Loss: 35.1633\n",
      "test, Batch: 3600/4779 Loss: 34.8652\n",
      "test, Batch: 3640/4779 Loss: 34.4844\n",
      "test, Batch: 3680/4779 Loss: 34.1113\n",
      "test, Batch: 3720/4779 Loss: 33.7468\n",
      "test, Batch: 3760/4779 Loss: 33.8188\n",
      "test, Batch: 3800/4779 Loss: 33.4643\n",
      "test, Batch: 3840/4779 Loss: 33.1179\n",
      "test, Batch: 3880/4779 Loss: 32.7789\n",
      "test, Batch: 3920/4779 Loss: 35.2243\n",
      "test, Batch: 3960/4779 Loss: 34.8696\n",
      "test, Batch: 4000/4779 Loss: 34.5736\n",
      "test, Batch: 4040/4779 Loss: 34.3351\n",
      "test, Batch: 4080/4779 Loss: 34.0005\n",
      "test, Batch: 4120/4779 Loss: 33.6751\n",
      "test, Batch: 4160/4779 Loss: 33.3534\n",
      "test, Batch: 4200/4779 Loss: 33.0373\n",
      "test, Batch: 4240/4779 Loss: 33.6394\n",
      "test, Batch: 4280/4779 Loss: 33.4861\n",
      "test, Batch: 4320/4779 Loss: 33.2007\n",
      "test, Batch: 4360/4779 Loss: 32.9781\n",
      "test, Batch: 4400/4779 Loss: 32.6877\n",
      "test, Batch: 4440/4779 Loss: 32.5644\n",
      "test, Batch: 4480/4779 Loss: 32.3281\n",
      "test, Batch: 4520/4779 Loss: 32.0953\n",
      "test, Batch: 4560/4779 Loss: 31.8163\n",
      "test, Batch: 4600/4779 Loss: 31.5905\n",
      "test, Batch: 4640/4779 Loss: 31.3201\n",
      "test, Batch: 4680/4779 Loss: 33.6759\n",
      "test, Batch: 4720/4779 Loss: 33.4293\n",
      "test, Batch: 4760/4779 Loss: 33.1502\n",
      "Epoch 8/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.4014\n",
      "train, Batch: 40/2390 Loss: 89.1969\n",
      "train, Batch: 80/2390 Loss: 51.2795\n",
      "train, Batch: 120/2390 Loss: 43.4704\n",
      "train, Batch: 160/2390 Loss: 34.4935\n",
      "train, Batch: 200/2390 Loss: 28.2021\n",
      "train, Batch: 240/2390 Loss: 26.7218\n",
      "train, Batch: 280/2390 Loss: 34.7797\n",
      "train, Batch: 320/2390 Loss: 32.2574\n",
      "train, Batch: 360/2390 Loss: 34.2151\n",
      "train, Batch: 400/2390 Loss: 32.6784\n",
      "train, Batch: 440/2390 Loss: 30.3166\n",
      "train, Batch: 480/2390 Loss: 31.7441\n",
      "train, Batch: 520/2390 Loss: 29.5907\n",
      "train, Batch: 560/2390 Loss: 30.4967\n",
      "train, Batch: 600/2390 Loss: 29.9630\n",
      "train, Batch: 640/2390 Loss: 29.9755\n",
      "train, Batch: 680/2390 Loss: 30.3219\n",
      "train, Batch: 720/2390 Loss: 29.6233\n",
      "train, Batch: 760/2390 Loss: 28.8132\n",
      "train, Batch: 800/2390 Loss: 27.6701\n",
      "train, Batch: 840/2390 Loss: 28.0379\n",
      "train, Batch: 880/2390 Loss: 28.0538\n",
      "train, Batch: 920/2390 Loss: 27.8434\n",
      "train, Batch: 960/2390 Loss: 26.8626\n",
      "train, Batch: 1000/2390 Loss: 26.0510\n",
      "train, Batch: 1040/2390 Loss: 29.6025\n",
      "train, Batch: 1080/2390 Loss: 29.1006\n",
      "train, Batch: 1120/2390 Loss: 30.5311\n",
      "train, Batch: 1160/2390 Loss: 29.5467\n",
      "train, Batch: 1200/2390 Loss: 28.7790\n",
      "train, Batch: 1240/2390 Loss: 28.5642\n",
      "train, Batch: 1280/2390 Loss: 29.0302\n",
      "train, Batch: 1320/2390 Loss: 29.5703\n",
      "train, Batch: 1360/2390 Loss: 28.8716\n",
      "train, Batch: 1400/2390 Loss: 28.3985\n",
      "train, Batch: 1440/2390 Loss: 27.8737\n",
      "train, Batch: 1480/2390 Loss: 27.9295\n",
      "train, Batch: 1520/2390 Loss: 28.0049\n",
      "train, Batch: 1560/2390 Loss: 27.7187\n",
      "train, Batch: 1600/2390 Loss: 27.8498\n",
      "train, Batch: 1640/2390 Loss: 27.3676\n",
      "train, Batch: 1680/2390 Loss: 26.8070\n",
      "train, Batch: 1720/2390 Loss: 26.2578\n",
      "train, Batch: 1760/2390 Loss: 26.0430\n",
      "train, Batch: 1800/2390 Loss: 25.5353\n",
      "train, Batch: 1840/2390 Loss: 25.0437\n",
      "train, Batch: 1880/2390 Loss: 25.0766\n",
      "train, Batch: 1920/2390 Loss: 24.9692\n",
      "train, Batch: 1960/2390 Loss: 24.6504\n",
      "train, Batch: 2000/2390 Loss: 24.7999\n",
      "train, Batch: 2040/2390 Loss: 24.7042\n",
      "train, Batch: 2080/2390 Loss: 25.1899\n",
      "train, Batch: 2120/2390 Loss: 24.9991\n",
      "train, Batch: 2160/2390 Loss: 24.5858\n",
      "train, Batch: 2200/2390 Loss: 24.7722\n",
      "train, Batch: 2240/2390 Loss: 25.6310\n",
      "train, Batch: 2280/2390 Loss: 26.2796\n",
      "train, Batch: 2320/2390 Loss: 26.6686\n",
      "train, Batch: 2360/2390 Loss: 26.4489\n",
      "test, Batch: 0/4779 Loss: 0.0406\n",
      "test, Batch: 40/4779 Loss: 8.2111\n",
      "test, Batch: 80/4779 Loss: 4.2143\n",
      "test, Batch: 120/4779 Loss: 6.7527\n",
      "test, Batch: 160/4779 Loss: 29.0062\n",
      "test, Batch: 200/4779 Loss: 24.9099\n",
      "test, Batch: 240/4779 Loss: 21.3206\n",
      "test, Batch: 280/4779 Loss: 18.3150\n",
      "test, Batch: 320/4779 Loss: 16.0581\n",
      "test, Batch: 360/4779 Loss: 19.9729\n",
      "test, Batch: 400/4779 Loss: 17.9987\n",
      "test, Batch: 440/4779 Loss: 16.8717\n",
      "test, Batch: 480/4779 Loss: 15.4795\n",
      "test, Batch: 520/4779 Loss: 14.3003\n",
      "test, Batch: 560/4779 Loss: 13.2916\n",
      "test, Batch: 600/4779 Loss: 12.4183\n",
      "test, Batch: 640/4779 Loss: 11.8347\n",
      "test, Batch: 680/4779 Loss: 24.2005\n",
      "test, Batch: 720/4779 Loss: 23.5937\n",
      "test, Batch: 760/4779 Loss: 22.3620\n",
      "test, Batch: 800/4779 Loss: 34.9082\n",
      "test, Batch: 840/4779 Loss: 38.3577\n",
      "test, Batch: 880/4779 Loss: 36.8114\n",
      "test, Batch: 920/4779 Loss: 35.3656\n",
      "test, Batch: 960/4779 Loss: 33.9015\n",
      "test, Batch: 1000/4779 Loss: 47.5425\n",
      "test, Batch: 1040/4779 Loss: 49.1579\n",
      "test, Batch: 1080/4779 Loss: 47.3503\n",
      "test, Batch: 1120/4779 Loss: 45.8119\n",
      "test, Batch: 1160/4779 Loss: 47.6452\n",
      "test, Batch: 1200/4779 Loss: 46.2398\n",
      "test, Batch: 1240/4779 Loss: 44.7560\n",
      "test, Batch: 1280/4779 Loss: 47.3025\n",
      "test, Batch: 1320/4779 Loss: 45.8754\n",
      "test, Batch: 1360/4779 Loss: 44.5317\n",
      "test, Batch: 1400/4779 Loss: 43.7418\n",
      "test, Batch: 1440/4779 Loss: 42.5327\n",
      "test, Batch: 1480/4779 Loss: 41.3890\n",
      "test, Batch: 1520/4779 Loss: 40.6389\n",
      "test, Batch: 1560/4779 Loss: 39.6058\n",
      "test, Batch: 1600/4779 Loss: 38.6205\n",
      "test, Batch: 1640/4779 Loss: 38.0284\n",
      "test, Batch: 1680/4779 Loss: 37.2191\n",
      "test, Batch: 1720/4779 Loss: 36.3602\n",
      "test, Batch: 1760/4779 Loss: 44.6349\n",
      "test, Batch: 1800/4779 Loss: 43.7772\n",
      "test, Batch: 1840/4779 Loss: 42.8316\n",
      "test, Batch: 1880/4779 Loss: 42.1011\n",
      "test, Batch: 1920/4779 Loss: 41.2266\n",
      "test, Batch: 1960/4779 Loss: 40.4923\n",
      "test, Batch: 2000/4779 Loss: 41.4638\n",
      "test, Batch: 2040/4779 Loss: 40.7891\n",
      "test, Batch: 2080/4779 Loss: 40.4152\n",
      "test, Batch: 2120/4779 Loss: 39.6574\n",
      "test, Batch: 2160/4779 Loss: 38.9266\n",
      "test, Batch: 2200/4779 Loss: 38.2217\n",
      "test, Batch: 2240/4779 Loss: 37.6637\n",
      "test, Batch: 2280/4779 Loss: 37.0066\n",
      "test, Batch: 2320/4779 Loss: 36.3734\n",
      "test, Batch: 2360/4779 Loss: 35.8339\n",
      "test, Batch: 2400/4779 Loss: 35.3290\n",
      "test, Batch: 2440/4779 Loss: 34.8406\n",
      "test, Batch: 2480/4779 Loss: 34.2825\n",
      "test, Batch: 2520/4779 Loss: 33.7412\n",
      "test, Batch: 2560/4779 Loss: 33.2167\n",
      "test, Batch: 2600/4779 Loss: 32.7090\n",
      "test, Batch: 2640/4779 Loss: 32.2653\n",
      "test, Batch: 2680/4779 Loss: 31.7862\n",
      "test, Batch: 2720/4779 Loss: 31.4039\n",
      "test, Batch: 2760/4779 Loss: 30.9934\n",
      "test, Batch: 2800/4779 Loss: 30.6800\n",
      "test, Batch: 2840/4779 Loss: 30.3258\n",
      "test, Batch: 2880/4779 Loss: 29.9063\n",
      "test, Batch: 2920/4779 Loss: 29.6476\n",
      "test, Batch: 2960/4779 Loss: 29.2493\n",
      "test, Batch: 3000/4779 Loss: 28.8619\n",
      "test, Batch: 3040/4779 Loss: 28.7254\n",
      "test, Batch: 3080/4779 Loss: 30.3174\n",
      "test, Batch: 3120/4779 Loss: 30.0023\n",
      "test, Batch: 3160/4779 Loss: 30.0579\n",
      "test, Batch: 3200/4779 Loss: 29.7555\n",
      "test, Batch: 3240/4779 Loss: 29.3908\n",
      "test, Batch: 3280/4779 Loss: 30.1246\n",
      "test, Batch: 3320/4779 Loss: 31.2172\n",
      "test, Batch: 3360/4779 Loss: 34.1255\n",
      "test, Batch: 3400/4779 Loss: 35.0473\n",
      "test, Batch: 3440/4779 Loss: 38.8800\n",
      "test, Batch: 3480/4779 Loss: 38.4691\n",
      "test, Batch: 3520/4779 Loss: 38.1325\n",
      "test, Batch: 3560/4779 Loss: 37.7054\n",
      "test, Batch: 3600/4779 Loss: 37.2878\n",
      "test, Batch: 3640/4779 Loss: 36.9256\n",
      "test, Batch: 3680/4779 Loss: 36.5274\n",
      "test, Batch: 3720/4779 Loss: 36.1409\n",
      "test, Batch: 3760/4779 Loss: 39.5260\n",
      "test, Batch: 3800/4779 Loss: 39.1746\n",
      "test, Batch: 3840/4779 Loss: 38.7721\n",
      "test, Batch: 3880/4779 Loss: 39.2339\n",
      "test, Batch: 3920/4779 Loss: 38.8481\n",
      "test, Batch: 3960/4779 Loss: 38.4770\n",
      "test, Batch: 4000/4779 Loss: 38.0937\n",
      "test, Batch: 4040/4779 Loss: 37.7176\n",
      "test, Batch: 4080/4779 Loss: 37.4109\n",
      "test, Batch: 4120/4779 Loss: 37.0879\n",
      "test, Batch: 4160/4779 Loss: 36.7332\n",
      "test, Batch: 4200/4779 Loss: 36.4273\n",
      "test, Batch: 4240/4779 Loss: 38.0874\n",
      "test, Batch: 4280/4779 Loss: 37.7339\n",
      "test, Batch: 4320/4779 Loss: 37.3864\n",
      "test, Batch: 4360/4779 Loss: 37.0448\n",
      "test, Batch: 4400/4779 Loss: 36.7097\n",
      "test, Batch: 4440/4779 Loss: 36.3803\n",
      "test, Batch: 4480/4779 Loss: 36.3932\n",
      "test, Batch: 4520/4779 Loss: 36.0730\n",
      "test, Batch: 4560/4779 Loss: 35.7675\n",
      "test, Batch: 4600/4779 Loss: 35.4580\n",
      "test, Batch: 4640/4779 Loss: 35.1988\n",
      "test, Batch: 4680/4779 Loss: 34.8992\n",
      "test, Batch: 4720/4779 Loss: 34.6060\n",
      "test, Batch: 4760/4779 Loss: 36.5373\n",
      "Epoch 9/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.2064\n",
      "train, Batch: 40/2390 Loss: 30.3401\n",
      "train, Batch: 80/2390 Loss: 21.6351\n",
      "train, Batch: 120/2390 Loss: 28.3626\n",
      "train, Batch: 160/2390 Loss: 29.7888\n",
      "train, Batch: 200/2390 Loss: 25.5022\n",
      "train, Batch: 240/2390 Loss: 25.8054\n",
      "train, Batch: 280/2390 Loss: 24.7418\n",
      "train, Batch: 320/2390 Loss: 22.0498\n",
      "train, Batch: 360/2390 Loss: 27.9131\n",
      "train, Batch: 400/2390 Loss: 29.1799\n",
      "train, Batch: 440/2390 Loss: 32.8443\n",
      "train, Batch: 480/2390 Loss: 35.5825\n",
      "train, Batch: 520/2390 Loss: 36.3544\n",
      "train, Batch: 560/2390 Loss: 34.0113\n",
      "train, Batch: 600/2390 Loss: 31.9461\n",
      "train, Batch: 640/2390 Loss: 31.2900\n",
      "train, Batch: 680/2390 Loss: 30.0295\n",
      "train, Batch: 720/2390 Loss: 29.2866\n",
      "train, Batch: 760/2390 Loss: 29.6241\n",
      "train, Batch: 800/2390 Loss: 28.1877\n",
      "train, Batch: 840/2390 Loss: 27.0227\n",
      "train, Batch: 880/2390 Loss: 25.9511\n",
      "train, Batch: 920/2390 Loss: 29.7567\n",
      "train, Batch: 960/2390 Loss: 28.5627\n",
      "train, Batch: 1000/2390 Loss: 27.5003\n",
      "train, Batch: 1040/2390 Loss: 26.5129\n",
      "train, Batch: 1080/2390 Loss: 25.7915\n",
      "train, Batch: 1120/2390 Loss: 24.9942\n",
      "train, Batch: 1160/2390 Loss: 24.5750\n",
      "train, Batch: 1200/2390 Loss: 24.1679\n",
      "train, Batch: 1240/2390 Loss: 23.4701\n",
      "train, Batch: 1280/2390 Loss: 23.1705\n",
      "train, Batch: 1320/2390 Loss: 22.8241\n",
      "train, Batch: 1360/2390 Loss: 22.3552\n",
      "train, Batch: 1400/2390 Loss: 23.3321\n",
      "train, Batch: 1440/2390 Loss: 23.0024\n",
      "train, Batch: 1480/2390 Loss: 22.3941\n",
      "train, Batch: 1520/2390 Loss: 24.3696\n",
      "train, Batch: 1560/2390 Loss: 25.9383\n",
      "train, Batch: 1600/2390 Loss: 26.1744\n",
      "train, Batch: 1640/2390 Loss: 25.7899\n",
      "train, Batch: 1680/2390 Loss: 25.7526\n",
      "train, Batch: 1720/2390 Loss: 25.2395\n",
      "train, Batch: 1760/2390 Loss: 25.7475\n",
      "train, Batch: 1800/2390 Loss: 26.9343\n",
      "train, Batch: 1840/2390 Loss: 26.6685\n",
      "train, Batch: 1880/2390 Loss: 27.4895\n",
      "train, Batch: 1920/2390 Loss: 27.0767\n",
      "train, Batch: 1960/2390 Loss: 26.6807\n",
      "train, Batch: 2000/2390 Loss: 26.7910\n",
      "train, Batch: 2040/2390 Loss: 26.4271\n",
      "train, Batch: 2080/2390 Loss: 26.2016\n",
      "train, Batch: 2120/2390 Loss: 27.0015\n",
      "train, Batch: 2160/2390 Loss: 26.6280\n",
      "train, Batch: 2200/2390 Loss: 27.0592\n",
      "train, Batch: 2240/2390 Loss: 26.7800\n",
      "train, Batch: 2280/2390 Loss: 26.5784\n",
      "train, Batch: 2320/2390 Loss: 26.3181\n",
      "train, Batch: 2360/2390 Loss: 26.0664\n",
      "test, Batch: 0/4779 Loss: 0.0348\n",
      "test, Batch: 40/4779 Loss: 15.4786\n",
      "test, Batch: 80/4779 Loss: 10.4207\n",
      "test, Batch: 120/4779 Loss: 50.7783\n",
      "test, Batch: 160/4779 Loss: 38.1921\n",
      "test, Batch: 200/4779 Loss: 30.6452\n",
      "test, Batch: 240/4779 Loss: 25.7446\n",
      "test, Batch: 280/4779 Loss: 22.0964\n",
      "test, Batch: 320/4779 Loss: 19.3636\n",
      "test, Batch: 360/4779 Loss: 30.7703\n",
      "test, Batch: 400/4779 Loss: 27.7136\n",
      "test, Batch: 440/4779 Loss: 25.6763\n",
      "test, Batch: 480/4779 Loss: 24.3166\n",
      "test, Batch: 520/4779 Loss: 25.1329\n",
      "test, Batch: 560/4779 Loss: 24.2348\n",
      "test, Batch: 600/4779 Loss: 22.6374\n",
      "test, Batch: 640/4779 Loss: 21.2330\n",
      "test, Batch: 680/4779 Loss: 24.4439\n",
      "test, Batch: 720/4779 Loss: 23.0950\n",
      "test, Batch: 760/4779 Loss: 21.8887\n",
      "test, Batch: 800/4779 Loss: 23.8663\n",
      "test, Batch: 840/4779 Loss: 22.7362\n",
      "test, Batch: 880/4779 Loss: 21.7086\n",
      "test, Batch: 920/4779 Loss: 21.7245\n",
      "test, Batch: 960/4779 Loss: 20.8269\n",
      "test, Batch: 1000/4779 Loss: 20.0021\n",
      "test, Batch: 1040/4779 Loss: 19.2385\n",
      "test, Batch: 1080/4779 Loss: 18.6739\n",
      "test, Batch: 1120/4779 Loss: 19.0530\n",
      "test, Batch: 1160/4779 Loss: 18.5984\n",
      "test, Batch: 1200/4779 Loss: 17.9846\n",
      "test, Batch: 1240/4779 Loss: 21.2218\n",
      "test, Batch: 1280/4779 Loss: 29.4017\n",
      "test, Batch: 1320/4779 Loss: 28.5177\n",
      "test, Batch: 1360/4779 Loss: 27.7612\n",
      "test, Batch: 1400/4779 Loss: 26.9732\n",
      "test, Batch: 1440/4779 Loss: 26.2291\n",
      "test, Batch: 1480/4779 Loss: 25.6972\n",
      "test, Batch: 1520/4779 Loss: 33.2550\n",
      "test, Batch: 1560/4779 Loss: 32.4084\n",
      "test, Batch: 1600/4779 Loss: 31.7573\n",
      "test, Batch: 1640/4779 Loss: 33.2189\n",
      "test, Batch: 1680/4779 Loss: 33.0799\n",
      "test, Batch: 1720/4779 Loss: 32.3136\n",
      "test, Batch: 1760/4779 Loss: 31.5922\n",
      "test, Batch: 1800/4779 Loss: 39.1092\n",
      "test, Batch: 1840/4779 Loss: 38.2627\n",
      "test, Batch: 1880/4779 Loss: 37.4534\n",
      "test, Batch: 1920/4779 Loss: 36.6759\n",
      "test, Batch: 1960/4779 Loss: 38.4498\n",
      "test, Batch: 2000/4779 Loss: 39.6809\n",
      "test, Batch: 2040/4779 Loss: 39.2920\n",
      "test, Batch: 2080/4779 Loss: 38.6350\n",
      "test, Batch: 2120/4779 Loss: 37.9113\n",
      "test, Batch: 2160/4779 Loss: 37.2723\n",
      "test, Batch: 2200/4779 Loss: 40.8976\n",
      "test, Batch: 2240/4779 Loss: 40.1701\n",
      "test, Batch: 2280/4779 Loss: 39.4739\n",
      "test, Batch: 2320/4779 Loss: 39.2301\n",
      "test, Batch: 2360/4779 Loss: 38.5685\n",
      "test, Batch: 2400/4779 Loss: 42.6817\n",
      "test, Batch: 2440/4779 Loss: 48.5005\n",
      "test, Batch: 2480/4779 Loss: 47.7219\n",
      "test, Batch: 2520/4779 Loss: 47.0154\n",
      "test, Batch: 2560/4779 Loss: 46.4380\n",
      "test, Batch: 2600/4779 Loss: 48.2944\n",
      "test, Batch: 2640/4779 Loss: 47.5651\n",
      "test, Batch: 2680/4779 Loss: 46.8585\n",
      "test, Batch: 2720/4779 Loss: 47.5294\n",
      "test, Batch: 2760/4779 Loss: 46.8431\n",
      "test, Batch: 2800/4779 Loss: 46.1767\n",
      "test, Batch: 2840/4779 Loss: 45.5298\n",
      "test, Batch: 2880/4779 Loss: 45.4232\n",
      "test, Batch: 2920/4779 Loss: 44.8335\n",
      "test, Batch: 2960/4779 Loss: 44.2859\n",
      "test, Batch: 3000/4779 Loss: 43.9525\n",
      "test, Batch: 3040/4779 Loss: 43.3782\n",
      "test, Batch: 3080/4779 Loss: 42.8169\n",
      "test, Batch: 3120/4779 Loss: 42.3192\n",
      "test, Batch: 3160/4779 Loss: 44.6004\n",
      "test, Batch: 3200/4779 Loss: 44.0451\n",
      "test, Batch: 3240/4779 Loss: 43.5100\n",
      "test, Batch: 3280/4779 Loss: 43.3461\n",
      "test, Batch: 3320/4779 Loss: 43.3621\n",
      "test, Batch: 3360/4779 Loss: 42.8489\n",
      "test, Batch: 3400/4779 Loss: 42.6243\n",
      "test, Batch: 3440/4779 Loss: 42.1804\n",
      "test, Batch: 3480/4779 Loss: 41.6995\n",
      "test, Batch: 3520/4779 Loss: 41.2855\n",
      "test, Batch: 3560/4779 Loss: 40.8230\n",
      "test, Batch: 3600/4779 Loss: 40.4058\n",
      "test, Batch: 3640/4779 Loss: 40.0213\n",
      "test, Batch: 3680/4779 Loss: 39.5888\n",
      "test, Batch: 3720/4779 Loss: 39.1649\n",
      "test, Batch: 3760/4779 Loss: 38.7504\n",
      "test, Batch: 3800/4779 Loss: 41.1736\n",
      "test, Batch: 3840/4779 Loss: 40.7761\n",
      "test, Batch: 3880/4779 Loss: 40.4673\n",
      "test, Batch: 3920/4779 Loss: 40.3006\n",
      "test, Batch: 3960/4779 Loss: 40.1020\n",
      "test, Batch: 4000/4779 Loss: 39.7022\n",
      "test, Batch: 4040/4779 Loss: 42.1040\n",
      "test, Batch: 4080/4779 Loss: 41.6940\n",
      "test, Batch: 4120/4779 Loss: 41.2905\n",
      "test, Batch: 4160/4779 Loss: 40.9249\n",
      "test, Batch: 4200/4779 Loss: 40.5364\n",
      "test, Batch: 4240/4779 Loss: 40.2587\n",
      "test, Batch: 4280/4779 Loss: 41.3131\n",
      "test, Batch: 4320/4779 Loss: 41.6284\n",
      "test, Batch: 4360/4779 Loss: 41.2483\n",
      "test, Batch: 4400/4779 Loss: 40.9213\n",
      "test, Batch: 4440/4779 Loss: 40.5535\n",
      "test, Batch: 4480/4779 Loss: 40.2249\n",
      "test, Batch: 4520/4779 Loss: 39.8730\n",
      "test, Batch: 4560/4779 Loss: 39.5250\n",
      "test, Batch: 4600/4779 Loss: 39.1827\n",
      "test, Batch: 4640/4779 Loss: 38.8476\n",
      "test, Batch: 4680/4779 Loss: 38.5592\n",
      "test, Batch: 4720/4779 Loss: 38.2956\n",
      "test, Batch: 4760/4779 Loss: 37.9752\n",
      "Epoch 10/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 19.2560\n",
      "train, Batch: 40/2390 Loss: 24.2946\n",
      "train, Batch: 80/2390 Loss: 43.6812\n",
      "train, Batch: 120/2390 Loss: 31.2116\n",
      "train, Batch: 160/2390 Loss: 38.5495\n",
      "train, Batch: 200/2390 Loss: 39.3549\n",
      "train, Batch: 240/2390 Loss: 41.0179\n",
      "train, Batch: 280/2390 Loss: 39.4709\n",
      "train, Batch: 320/2390 Loss: 35.6069\n",
      "train, Batch: 360/2390 Loss: 34.6949\n",
      "train, Batch: 400/2390 Loss: 39.2617\n",
      "train, Batch: 440/2390 Loss: 39.2359\n",
      "train, Batch: 480/2390 Loss: 42.7461\n",
      "train, Batch: 520/2390 Loss: 39.9125\n",
      "train, Batch: 560/2390 Loss: 42.1359\n",
      "train, Batch: 600/2390 Loss: 39.4502\n",
      "train, Batch: 640/2390 Loss: 38.3586\n",
      "train, Batch: 680/2390 Loss: 37.3489\n",
      "train, Batch: 720/2390 Loss: 35.5010\n",
      "train, Batch: 760/2390 Loss: 33.7712\n",
      "train, Batch: 800/2390 Loss: 32.2675\n",
      "train, Batch: 840/2390 Loss: 35.7601\n",
      "train, Batch: 880/2390 Loss: 36.2235\n",
      "train, Batch: 920/2390 Loss: 35.2354\n",
      "train, Batch: 960/2390 Loss: 34.1843\n",
      "train, Batch: 1000/2390 Loss: 34.8312\n",
      "train, Batch: 1040/2390 Loss: 33.5084\n",
      "train, Batch: 1080/2390 Loss: 32.2920\n",
      "train, Batch: 1120/2390 Loss: 31.6437\n",
      "train, Batch: 1160/2390 Loss: 30.7637\n",
      "train, Batch: 1200/2390 Loss: 30.1442\n",
      "train, Batch: 1240/2390 Loss: 30.2551\n",
      "train, Batch: 1280/2390 Loss: 30.2513\n",
      "train, Batch: 1320/2390 Loss: 29.5466\n",
      "train, Batch: 1360/2390 Loss: 28.8129\n",
      "train, Batch: 1400/2390 Loss: 28.6667\n",
      "train, Batch: 1440/2390 Loss: 28.9335\n",
      "train, Batch: 1480/2390 Loss: 29.6921\n",
      "train, Batch: 1520/2390 Loss: 29.3902\n",
      "train, Batch: 1560/2390 Loss: 28.9955\n",
      "train, Batch: 1600/2390 Loss: 28.5368\n",
      "train, Batch: 1640/2390 Loss: 29.6109\n",
      "train, Batch: 1680/2390 Loss: 28.9275\n",
      "train, Batch: 1720/2390 Loss: 28.3528\n",
      "train, Batch: 1760/2390 Loss: 28.0241\n",
      "train, Batch: 1800/2390 Loss: 27.6400\n",
      "train, Batch: 1840/2390 Loss: 28.1226\n",
      "train, Batch: 1880/2390 Loss: 27.6173\n",
      "train, Batch: 1920/2390 Loss: 27.4394\n",
      "train, Batch: 1960/2390 Loss: 26.9719\n",
      "train, Batch: 2000/2390 Loss: 26.5254\n",
      "train, Batch: 2040/2390 Loss: 26.2263\n",
      "train, Batch: 2080/2390 Loss: 26.6798\n",
      "train, Batch: 2120/2390 Loss: 26.2503\n",
      "train, Batch: 2160/2390 Loss: 26.3505\n",
      "train, Batch: 2200/2390 Loss: 26.2447\n",
      "train, Batch: 2240/2390 Loss: 25.9164\n",
      "train, Batch: 2280/2390 Loss: 26.6811\n",
      "train, Batch: 2320/2390 Loss: 26.6341\n",
      "train, Batch: 2360/2390 Loss: 26.3655\n",
      "test, Batch: 0/4779 Loss: 0.0419\n",
      "test, Batch: 40/4779 Loss: 0.1686\n",
      "test, Batch: 80/4779 Loss: 0.1644\n",
      "test, Batch: 120/4779 Loss: 62.4352\n",
      "test, Batch: 160/4779 Loss: 47.1320\n",
      "test, Batch: 200/4779 Loss: 37.8451\n",
      "test, Batch: 240/4779 Loss: 32.3169\n",
      "test, Batch: 280/4779 Loss: 27.7384\n",
      "test, Batch: 320/4779 Loss: 24.3006\n",
      "test, Batch: 360/4779 Loss: 21.6362\n",
      "test, Batch: 400/4779 Loss: 19.5008\n",
      "test, Batch: 440/4779 Loss: 21.6235\n",
      "test, Batch: 480/4779 Loss: 20.2237\n",
      "test, Batch: 520/4779 Loss: 18.6966\n",
      "test, Batch: 560/4779 Loss: 24.9484\n",
      "test, Batch: 600/4779 Loss: 23.4591\n",
      "test, Batch: 640/4779 Loss: 22.0223\n",
      "test, Batch: 680/4779 Loss: 21.3493\n",
      "test, Batch: 720/4779 Loss: 20.3014\n",
      "test, Batch: 760/4779 Loss: 19.9064\n",
      "test, Batch: 800/4779 Loss: 18.9185\n",
      "test, Batch: 840/4779 Loss: 18.8557\n",
      "test, Batch: 880/4779 Loss: 18.3984\n",
      "test, Batch: 920/4779 Loss: 17.8542\n",
      "test, Batch: 960/4779 Loss: 17.1333\n",
      "test, Batch: 1000/4779 Loss: 16.4595\n",
      "test, Batch: 1040/4779 Loss: 15.8373\n",
      "test, Batch: 1080/4779 Loss: 15.2559\n",
      "test, Batch: 1120/4779 Loss: 14.7210\n",
      "test, Batch: 1160/4779 Loss: 14.2220\n",
      "test, Batch: 1200/4779 Loss: 13.9054\n",
      "test, Batch: 1240/4779 Loss: 13.4621\n",
      "test, Batch: 1280/4779 Loss: 13.6306\n",
      "test, Batch: 1320/4779 Loss: 13.5363\n",
      "test, Batch: 1360/4779 Loss: 13.2420\n",
      "test, Batch: 1400/4779 Loss: 12.8687\n",
      "test, Batch: 1440/4779 Loss: 12.5160\n",
      "test, Batch: 1480/4779 Loss: 12.1835\n",
      "test, Batch: 1520/4779 Loss: 15.1464\n",
      "test, Batch: 1560/4779 Loss: 14.7918\n",
      "test, Batch: 1600/4779 Loss: 15.0250\n",
      "test, Batch: 1640/4779 Loss: 14.6649\n",
      "test, Batch: 1680/4779 Loss: 14.5391\n",
      "test, Batch: 1720/4779 Loss: 14.9741\n",
      "test, Batch: 1760/4779 Loss: 14.7428\n",
      "test, Batch: 1800/4779 Loss: 14.4232\n",
      "test, Batch: 1840/4779 Loss: 14.1143\n",
      "test, Batch: 1880/4779 Loss: 13.8205\n",
      "test, Batch: 1920/4779 Loss: 23.0681\n",
      "test, Batch: 1960/4779 Loss: 22.8403\n",
      "test, Batch: 2000/4779 Loss: 22.3915\n",
      "test, Batch: 2040/4779 Loss: 22.2928\n",
      "test, Batch: 2080/4779 Loss: 22.0709\n",
      "test, Batch: 2120/4779 Loss: 21.6582\n",
      "test, Batch: 2160/4779 Loss: 22.7038\n",
      "test, Batch: 2200/4779 Loss: 22.2938\n",
      "test, Batch: 2240/4779 Loss: 26.1424\n",
      "test, Batch: 2280/4779 Loss: 25.6869\n",
      "test, Batch: 2320/4779 Loss: 25.2499\n",
      "test, Batch: 2360/4779 Loss: 25.4754\n",
      "test, Batch: 2400/4779 Loss: 25.0584\n",
      "test, Batch: 2440/4779 Loss: 24.6520\n",
      "test, Batch: 2480/4779 Loss: 24.2622\n",
      "test, Batch: 2520/4779 Loss: 23.8795\n",
      "test, Batch: 2560/4779 Loss: 27.8401\n",
      "test, Batch: 2600/4779 Loss: 27.4141\n",
      "test, Batch: 2640/4779 Loss: 27.4899\n",
      "test, Batch: 2680/4779 Loss: 27.0835\n",
      "test, Batch: 2720/4779 Loss: 28.4353\n",
      "test, Batch: 2760/4779 Loss: 28.0255\n",
      "test, Batch: 2800/4779 Loss: 28.9438\n",
      "test, Batch: 2840/4779 Loss: 28.7336\n",
      "test, Batch: 2880/4779 Loss: 28.4004\n",
      "test, Batch: 2920/4779 Loss: 28.1554\n",
      "test, Batch: 2960/4779 Loss: 27.7777\n",
      "test, Batch: 3000/4779 Loss: 27.4090\n",
      "test, Batch: 3040/4779 Loss: 27.2892\n",
      "test, Batch: 3080/4779 Loss: 26.9973\n",
      "test, Batch: 3120/4779 Loss: 27.0484\n",
      "test, Batch: 3160/4779 Loss: 26.8000\n",
      "test, Batch: 3200/4779 Loss: 26.4672\n",
      "test, Batch: 3240/4779 Loss: 26.1427\n",
      "test, Batch: 3280/4779 Loss: 25.8267\n",
      "test, Batch: 3320/4779 Loss: 25.5182\n",
      "test, Batch: 3360/4779 Loss: 25.2162\n",
      "test, Batch: 3400/4779 Loss: 27.9194\n",
      "test, Batch: 3440/4779 Loss: 27.5973\n",
      "test, Batch: 3480/4779 Loss: 27.7389\n",
      "test, Batch: 3520/4779 Loss: 27.4260\n",
      "test, Batch: 3560/4779 Loss: 27.1740\n",
      "test, Batch: 3600/4779 Loss: 26.8737\n",
      "test, Batch: 3640/4779 Loss: 26.6088\n",
      "test, Batch: 3680/4779 Loss: 28.5607\n",
      "test, Batch: 3720/4779 Loss: 31.6599\n",
      "test, Batch: 3760/4779 Loss: 31.3810\n",
      "test, Batch: 3800/4779 Loss: 34.0935\n",
      "test, Batch: 3840/4779 Loss: 37.0308\n",
      "test, Batch: 3880/4779 Loss: 40.6585\n",
      "test, Batch: 3920/4779 Loss: 40.2657\n",
      "test, Batch: 3960/4779 Loss: 39.8874\n",
      "test, Batch: 4000/4779 Loss: 39.4906\n",
      "test, Batch: 4040/4779 Loss: 39.1012\n",
      "test, Batch: 4080/4779 Loss: 40.2386\n",
      "test, Batch: 4120/4779 Loss: 39.8805\n",
      "test, Batch: 4160/4779 Loss: 39.4984\n",
      "test, Batch: 4200/4779 Loss: 39.1237\n",
      "test, Batch: 4240/4779 Loss: 38.8508\n",
      "test, Batch: 4280/4779 Loss: 38.5827\n",
      "test, Batch: 4320/4779 Loss: 38.2770\n",
      "test, Batch: 4360/4779 Loss: 37.9271\n",
      "test, Batch: 4400/4779 Loss: 37.5834\n",
      "test, Batch: 4440/4779 Loss: 37.2465\n",
      "test, Batch: 4480/4779 Loss: 36.9153\n",
      "test, Batch: 4520/4779 Loss: 36.5905\n",
      "test, Batch: 4560/4779 Loss: 36.2959\n",
      "test, Batch: 4600/4779 Loss: 36.1679\n",
      "test, Batch: 4640/4779 Loss: 36.1900\n",
      "test, Batch: 4680/4779 Loss: 36.8288\n",
      "test, Batch: 4720/4779 Loss: 36.5186\n",
      "test, Batch: 4760/4779 Loss: 36.2137\n",
      "Epoch 11/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.4197\n",
      "train, Batch: 40/2390 Loss: 45.3452\n",
      "train, Batch: 80/2390 Loss: 28.8636\n",
      "train, Batch: 120/2390 Loss: 20.4604\n",
      "train, Batch: 160/2390 Loss: 17.0239\n",
      "train, Batch: 200/2390 Loss: 28.8910\n",
      "train, Batch: 240/2390 Loss: 25.3526\n",
      "train, Batch: 280/2390 Loss: 22.7132\n",
      "train, Batch: 320/2390 Loss: 21.7894\n",
      "train, Batch: 360/2390 Loss: 20.1411\n",
      "train, Batch: 400/2390 Loss: 18.5635\n",
      "train, Batch: 440/2390 Loss: 18.3959\n",
      "train, Batch: 480/2390 Loss: 17.3106\n",
      "train, Batch: 520/2390 Loss: 16.3130\n",
      "train, Batch: 560/2390 Loss: 17.8895\n",
      "train, Batch: 600/2390 Loss: 16.8415\n",
      "train, Batch: 640/2390 Loss: 15.8657\n",
      "train, Batch: 680/2390 Loss: 17.4501\n",
      "train, Batch: 720/2390 Loss: 18.2171\n",
      "train, Batch: 760/2390 Loss: 21.3941\n",
      "train, Batch: 800/2390 Loss: 22.5798\n",
      "train, Batch: 840/2390 Loss: 21.6071\n",
      "train, Batch: 880/2390 Loss: 22.0870\n",
      "train, Batch: 920/2390 Loss: 22.1801\n",
      "train, Batch: 960/2390 Loss: 22.3592\n",
      "train, Batch: 1000/2390 Loss: 21.6047\n",
      "train, Batch: 1040/2390 Loss: 20.9490\n",
      "train, Batch: 1080/2390 Loss: 22.2171\n",
      "train, Batch: 1120/2390 Loss: 23.2847\n",
      "train, Batch: 1160/2390 Loss: 22.8215\n",
      "train, Batch: 1200/2390 Loss: 22.4006\n",
      "train, Batch: 1240/2390 Loss: 23.1335\n",
      "train, Batch: 1280/2390 Loss: 23.9497\n",
      "train, Batch: 1320/2390 Loss: 23.5509\n",
      "train, Batch: 1360/2390 Loss: 22.9456\n",
      "train, Batch: 1400/2390 Loss: 24.6570\n",
      "train, Batch: 1440/2390 Loss: 25.1723\n",
      "train, Batch: 1480/2390 Loss: 24.6780\n",
      "train, Batch: 1520/2390 Loss: 24.0660\n",
      "train, Batch: 1560/2390 Loss: 23.4810\n",
      "train, Batch: 1600/2390 Loss: 25.0329\n",
      "train, Batch: 1640/2390 Loss: 24.6220\n",
      "train, Batch: 1680/2390 Loss: 24.0602\n",
      "train, Batch: 1720/2390 Loss: 23.9163\n",
      "train, Batch: 1760/2390 Loss: 24.8356\n",
      "train, Batch: 1800/2390 Loss: 26.3529\n",
      "train, Batch: 1840/2390 Loss: 25.8302\n",
      "train, Batch: 1880/2390 Loss: 25.3384\n",
      "train, Batch: 1920/2390 Loss: 26.7514\n",
      "train, Batch: 1960/2390 Loss: 26.3331\n",
      "train, Batch: 2000/2390 Loss: 27.0922\n",
      "train, Batch: 2040/2390 Loss: 26.8540\n",
      "train, Batch: 2080/2390 Loss: 26.4982\n",
      "train, Batch: 2120/2390 Loss: 26.5836\n",
      "train, Batch: 2160/2390 Loss: 27.0911\n",
      "train, Batch: 2200/2390 Loss: 26.7013\n",
      "train, Batch: 2240/2390 Loss: 26.6461\n",
      "train, Batch: 2280/2390 Loss: 26.7971\n",
      "train, Batch: 2320/2390 Loss: 26.5894\n",
      "train, Batch: 2360/2390 Loss: 26.4410\n",
      "test, Batch: 0/4779 Loss: 0.1111\n",
      "test, Batch: 40/4779 Loss: 7.8020\n",
      "test, Batch: 80/4779 Loss: 4.0700\n",
      "test, Batch: 120/4779 Loss: 2.8797\n",
      "test, Batch: 160/4779 Loss: 2.2222\n",
      "test, Batch: 200/4779 Loss: 1.8156\n",
      "test, Batch: 240/4779 Loss: 1.5448\n",
      "test, Batch: 280/4779 Loss: 2.3801\n",
      "test, Batch: 320/4779 Loss: 2.1067\n",
      "test, Batch: 360/4779 Loss: 1.8967\n",
      "test, Batch: 400/4779 Loss: 2.2108\n",
      "test, Batch: 440/4779 Loss: 2.0209\n",
      "test, Batch: 480/4779 Loss: 23.7877\n",
      "test, Batch: 520/4779 Loss: 22.3657\n",
      "test, Batch: 560/4779 Loss: 21.7441\n",
      "test, Batch: 600/4779 Loss: 20.6392\n",
      "test, Batch: 640/4779 Loss: 19.6297\n",
      "test, Batch: 680/4779 Loss: 18.4879\n",
      "test, Batch: 720/4779 Loss: 17.4725\n",
      "test, Batch: 760/4779 Loss: 16.8863\n",
      "test, Batch: 800/4779 Loss: 16.7091\n",
      "test, Batch: 840/4779 Loss: 15.9238\n",
      "test, Batch: 880/4779 Loss: 15.2080\n",
      "test, Batch: 920/4779 Loss: 14.5538\n",
      "test, Batch: 960/4779 Loss: 13.9534\n",
      "test, Batch: 1000/4779 Loss: 13.4109\n",
      "test, Batch: 1040/4779 Loss: 13.6548\n",
      "test, Batch: 1080/4779 Loss: 20.8781\n",
      "test, Batch: 1120/4779 Loss: 20.3167\n",
      "test, Batch: 1160/4779 Loss: 19.6240\n",
      "test, Batch: 1200/4779 Loss: 18.9756\n",
      "test, Batch: 1240/4779 Loss: 18.4701\n",
      "test, Batch: 1280/4779 Loss: 17.8972\n",
      "test, Batch: 1320/4779 Loss: 17.3746\n",
      "test, Batch: 1360/4779 Loss: 16.8703\n",
      "test, Batch: 1400/4779 Loss: 16.6278\n",
      "test, Batch: 1440/4779 Loss: 16.1714\n",
      "test, Batch: 1480/4779 Loss: 15.8669\n",
      "test, Batch: 1520/4779 Loss: 15.5190\n",
      "test, Batch: 1560/4779 Loss: 15.2026\n",
      "test, Batch: 1600/4779 Loss: 14.8287\n",
      "test, Batch: 1640/4779 Loss: 14.4710\n",
      "test, Batch: 1680/4779 Loss: 16.1098\n",
      "test, Batch: 1720/4779 Loss: 16.1898\n",
      "test, Batch: 1760/4779 Loss: 19.5927\n",
      "test, Batch: 1800/4779 Loss: 19.4134\n",
      "test, Batch: 1840/4779 Loss: 19.2407\n",
      "test, Batch: 1880/4779 Loss: 18.8352\n",
      "test, Batch: 1920/4779 Loss: 18.8032\n",
      "test, Batch: 1960/4779 Loss: 18.5889\n",
      "test, Batch: 2000/4779 Loss: 22.8844\n",
      "test, Batch: 2040/4779 Loss: 25.1453\n",
      "test, Batch: 2080/4779 Loss: 24.6672\n",
      "test, Batch: 2120/4779 Loss: 24.3224\n",
      "test, Batch: 2160/4779 Loss: 23.9271\n",
      "test, Batch: 2200/4779 Loss: 28.5681\n",
      "test, Batch: 2240/4779 Loss: 28.0609\n",
      "test, Batch: 2280/4779 Loss: 27.5710\n",
      "test, Batch: 2320/4779 Loss: 27.5789\n",
      "test, Batch: 2360/4779 Loss: 29.0849\n",
      "test, Batch: 2400/4779 Loss: 29.3046\n",
      "test, Batch: 2440/4779 Loss: 28.9265\n",
      "test, Batch: 2480/4779 Loss: 28.4636\n",
      "test, Batch: 2520/4779 Loss: 28.0143\n",
      "test, Batch: 2560/4779 Loss: 29.9758\n",
      "test, Batch: 2600/4779 Loss: 29.5950\n",
      "test, Batch: 2640/4779 Loss: 29.2055\n",
      "test, Batch: 2680/4779 Loss: 32.2003\n",
      "test, Batch: 2720/4779 Loss: 31.8003\n",
      "test, Batch: 2760/4779 Loss: 31.5187\n",
      "test, Batch: 2800/4779 Loss: 31.1063\n",
      "test, Batch: 2840/4779 Loss: 30.7379\n",
      "test, Batch: 2880/4779 Loss: 30.4698\n",
      "test, Batch: 2920/4779 Loss: 30.0551\n",
      "test, Batch: 2960/4779 Loss: 29.6515\n",
      "test, Batch: 3000/4779 Loss: 29.2588\n",
      "test, Batch: 3040/4779 Loss: 28.8759\n",
      "test, Batch: 3080/4779 Loss: 28.5823\n",
      "test, Batch: 3120/4779 Loss: 28.2192\n",
      "test, Batch: 3160/4779 Loss: 27.9415\n",
      "test, Batch: 3200/4779 Loss: 27.5941\n",
      "test, Batch: 3240/4779 Loss: 27.2557\n",
      "test, Batch: 3280/4779 Loss: 26.9260\n",
      "test, Batch: 3320/4779 Loss: 27.7536\n",
      "test, Batch: 3360/4779 Loss: 27.4669\n",
      "test, Batch: 3400/4779 Loss: 27.1465\n",
      "test, Batch: 3440/4779 Loss: 27.1148\n",
      "test, Batch: 3480/4779 Loss: 26.8052\n",
      "test, Batch: 3520/4779 Loss: 26.9607\n",
      "test, Batch: 3560/4779 Loss: 26.9607\n",
      "test, Batch: 3600/4779 Loss: 26.7417\n",
      "test, Batch: 3640/4779 Loss: 27.2492\n",
      "test, Batch: 3680/4779 Loss: 30.1259\n",
      "test, Batch: 3720/4779 Loss: 29.8042\n",
      "test, Batch: 3760/4779 Loss: 29.4891\n",
      "test, Batch: 3800/4779 Loss: 29.1808\n",
      "test, Batch: 3840/4779 Loss: 29.2879\n",
      "test, Batch: 3880/4779 Loss: 29.0188\n",
      "test, Batch: 3920/4779 Loss: 28.7253\n",
      "test, Batch: 3960/4779 Loss: 28.6055\n",
      "test, Batch: 4000/4779 Loss: 28.3217\n",
      "test, Batch: 4040/4779 Loss: 28.0475\n",
      "test, Batch: 4080/4779 Loss: 27.7741\n",
      "test, Batch: 4120/4779 Loss: 30.9991\n",
      "test, Batch: 4160/4779 Loss: 30.8306\n",
      "test, Batch: 4200/4779 Loss: 31.0482\n",
      "test, Batch: 4240/4779 Loss: 30.7568\n",
      "test, Batch: 4280/4779 Loss: 30.9082\n",
      "test, Batch: 4320/4779 Loss: 30.6235\n",
      "test, Batch: 4360/4779 Loss: 31.6447\n",
      "test, Batch: 4400/4779 Loss: 34.9494\n",
      "test, Batch: 4440/4779 Loss: 34.7167\n",
      "test, Batch: 4480/4779 Loss: 34.4079\n",
      "test, Batch: 4520/4779 Loss: 34.1484\n",
      "test, Batch: 4560/4779 Loss: 33.8506\n",
      "test, Batch: 4600/4779 Loss: 33.5577\n",
      "test, Batch: 4640/4779 Loss: 35.9836\n",
      "test, Batch: 4680/4779 Loss: 35.6785\n",
      "test, Batch: 4720/4779 Loss: 35.4098\n",
      "test, Batch: 4760/4779 Loss: 35.3226\n",
      "Epoch 12/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.2783\n",
      "train, Batch: 40/2390 Loss: 6.6866\n",
      "train, Batch: 80/2390 Loss: 12.3317\n",
      "train, Batch: 120/2390 Loss: 9.3327\n",
      "train, Batch: 160/2390 Loss: 10.7089\n",
      "train, Batch: 200/2390 Loss: 9.6422\n",
      "train, Batch: 240/2390 Loss: 11.2984\n",
      "train, Batch: 280/2390 Loss: 15.7249\n",
      "train, Batch: 320/2390 Loss: 14.7797\n",
      "train, Batch: 360/2390 Loss: 15.0723\n",
      "train, Batch: 400/2390 Loss: 13.9885\n",
      "train, Batch: 440/2390 Loss: 15.8409\n",
      "train, Batch: 480/2390 Loss: 16.5384\n",
      "train, Batch: 520/2390 Loss: 19.7597\n",
      "train, Batch: 560/2390 Loss: 19.8366\n",
      "train, Batch: 600/2390 Loss: 19.7135\n",
      "train, Batch: 640/2390 Loss: 25.9602\n",
      "train, Batch: 680/2390 Loss: 25.3973\n",
      "train, Batch: 720/2390 Loss: 24.3953\n",
      "train, Batch: 760/2390 Loss: 26.7484\n",
      "train, Batch: 800/2390 Loss: 25.4949\n",
      "train, Batch: 840/2390 Loss: 24.5862\n",
      "train, Batch: 880/2390 Loss: 24.7572\n",
      "train, Batch: 920/2390 Loss: 25.0643\n",
      "train, Batch: 960/2390 Loss: 25.7277\n",
      "train, Batch: 1000/2390 Loss: 24.8336\n",
      "train, Batch: 1040/2390 Loss: 25.3158\n",
      "train, Batch: 1080/2390 Loss: 25.7214\n",
      "train, Batch: 1120/2390 Loss: 24.8795\n",
      "train, Batch: 1160/2390 Loss: 24.1748\n",
      "train, Batch: 1200/2390 Loss: 23.8443\n",
      "train, Batch: 1240/2390 Loss: 23.2229\n",
      "train, Batch: 1280/2390 Loss: 22.7412\n",
      "train, Batch: 1320/2390 Loss: 22.3869\n",
      "train, Batch: 1360/2390 Loss: 22.0190\n",
      "train, Batch: 1400/2390 Loss: 22.3677\n",
      "train, Batch: 1440/2390 Loss: 21.8971\n",
      "train, Batch: 1480/2390 Loss: 21.3845\n",
      "train, Batch: 1520/2390 Loss: 21.1954\n",
      "train, Batch: 1560/2390 Loss: 21.1229\n",
      "train, Batch: 1600/2390 Loss: 22.3866\n",
      "train, Batch: 1640/2390 Loss: 23.9679\n",
      "train, Batch: 1680/2390 Loss: 23.4676\n",
      "train, Batch: 1720/2390 Loss: 23.0121\n",
      "train, Batch: 1760/2390 Loss: 23.4227\n",
      "train, Batch: 1800/2390 Loss: 23.0784\n",
      "train, Batch: 1840/2390 Loss: 23.9036\n",
      "train, Batch: 1880/2390 Loss: 23.5349\n",
      "train, Batch: 1920/2390 Loss: 23.1852\n",
      "train, Batch: 1960/2390 Loss: 23.5776\n",
      "train, Batch: 2000/2390 Loss: 23.5459\n",
      "train, Batch: 2040/2390 Loss: 23.2343\n",
      "train, Batch: 2080/2390 Loss: 23.6030\n",
      "train, Batch: 2120/2390 Loss: 23.6883\n",
      "train, Batch: 2160/2390 Loss: 23.4847\n",
      "train, Batch: 2200/2390 Loss: 23.1629\n",
      "train, Batch: 2240/2390 Loss: 23.5306\n",
      "train, Batch: 2280/2390 Loss: 24.8646\n",
      "train, Batch: 2320/2390 Loss: 24.9533\n",
      "train, Batch: 2360/2390 Loss: 25.3219\n",
      "test, Batch: 0/4779 Loss: 0.2781\n",
      "test, Batch: 40/4779 Loss: 0.1720\n",
      "test, Batch: 80/4779 Loss: 0.4899\n",
      "test, Batch: 120/4779 Loss: 2.0243\n",
      "test, Batch: 160/4779 Loss: 1.5829\n",
      "test, Batch: 200/4779 Loss: 1.3122\n",
      "test, Batch: 240/4779 Loss: 48.6191\n",
      "test, Batch: 280/4779 Loss: 41.7229\n",
      "test, Batch: 320/4779 Loss: 81.3112\n",
      "test, Batch: 360/4779 Loss: 72.3242\n",
      "test, Batch: 400/4779 Loss: 86.4871\n",
      "test, Batch: 440/4779 Loss: 79.1143\n",
      "test, Batch: 480/4779 Loss: 73.8840\n",
      "test, Batch: 520/4779 Loss: 68.2269\n",
      "test, Batch: 560/4779 Loss: 63.3721\n",
      "test, Batch: 600/4779 Loss: 61.7039\n",
      "test, Batch: 640/4779 Loss: 59.1838\n",
      "test, Batch: 680/4779 Loss: 56.0415\n",
      "test, Batch: 720/4779 Loss: 52.9423\n",
      "test, Batch: 760/4779 Loss: 50.3209\n",
      "test, Batch: 800/4779 Loss: 47.8227\n",
      "test, Batch: 840/4779 Loss: 65.0842\n",
      "test, Batch: 880/4779 Loss: 62.3689\n",
      "test, Batch: 920/4779 Loss: 59.6714\n",
      "test, Batch: 960/4779 Loss: 57.1974\n",
      "test, Batch: 1000/4779 Loss: 57.7591\n",
      "test, Batch: 1040/4779 Loss: 66.4942\n",
      "test, Batch: 1080/4779 Loss: 64.0716\n",
      "test, Batch: 1120/4779 Loss: 61.8015\n",
      "test, Batch: 1160/4779 Loss: 59.6803\n",
      "test, Batch: 1200/4779 Loss: 57.6987\n",
      "test, Batch: 1240/4779 Loss: 56.6171\n",
      "test, Batch: 1280/4779 Loss: 54.8540\n",
      "test, Batch: 1320/4779 Loss: 57.6423\n",
      "test, Batch: 1360/4779 Loss: 56.7568\n",
      "test, Batch: 1400/4779 Loss: 63.0375\n",
      "test, Batch: 1440/4779 Loss: 61.4608\n",
      "test, Batch: 1480/4779 Loss: 59.9524\n",
      "test, Batch: 1520/4779 Loss: 59.4017\n",
      "test, Batch: 1560/4779 Loss: 58.3183\n",
      "test, Batch: 1600/4779 Loss: 57.0018\n",
      "test, Batch: 1640/4779 Loss: 55.6255\n",
      "test, Batch: 1680/4779 Loss: 55.2142\n",
      "test, Batch: 1720/4779 Loss: 54.0578\n",
      "test, Batch: 1760/4779 Loss: 52.8943\n",
      "test, Batch: 1800/4779 Loss: 51.7245\n",
      "test, Batch: 1840/4779 Loss: 50.6053\n",
      "test, Batch: 1880/4779 Loss: 49.6436\n",
      "test, Batch: 1920/4779 Loss: 48.7498\n",
      "test, Batch: 1960/4779 Loss: 47.7597\n",
      "test, Batch: 2000/4779 Loss: 46.8092\n",
      "test, Batch: 2040/4779 Loss: 45.9711\n",
      "test, Batch: 2080/4779 Loss: 45.0925\n",
      "test, Batch: 2120/4779 Loss: 51.6248\n",
      "test, Batch: 2160/4779 Loss: 50.6747\n",
      "test, Batch: 2200/4779 Loss: 55.5547\n",
      "test, Batch: 2240/4779 Loss: 54.5659\n",
      "test, Batch: 2280/4779 Loss: 53.6117\n",
      "test, Batch: 2320/4779 Loss: 52.7555\n",
      "test, Batch: 2360/4779 Loss: 51.8650\n",
      "test, Batch: 2400/4779 Loss: 51.8941\n",
      "test, Batch: 2440/4779 Loss: 51.0485\n",
      "test, Batch: 2480/4779 Loss: 50.2760\n",
      "test, Batch: 2520/4779 Loss: 49.4806\n",
      "test, Batch: 2560/4779 Loss: 48.7103\n",
      "test, Batch: 2600/4779 Loss: 48.0665\n",
      "test, Batch: 2640/4779 Loss: 48.8034\n",
      "test, Batch: 2680/4779 Loss: 48.4308\n",
      "test, Batch: 2720/4779 Loss: 47.9255\n",
      "test, Batch: 2760/4779 Loss: 47.2351\n",
      "test, Batch: 2800/4779 Loss: 46.6390\n",
      "test, Batch: 2840/4779 Loss: 46.0299\n",
      "test, Batch: 2880/4779 Loss: 45.3928\n",
      "test, Batch: 2920/4779 Loss: 44.7748\n",
      "test, Batch: 2960/4779 Loss: 44.1735\n",
      "test, Batch: 3000/4779 Loss: 43.5889\n",
      "test, Batch: 3040/4779 Loss: 44.6076\n",
      "test, Batch: 3080/4779 Loss: 46.0109\n",
      "test, Batch: 3120/4779 Loss: 45.4232\n",
      "test, Batch: 3160/4779 Loss: 44.8509\n",
      "test, Batch: 3200/4779 Loss: 45.8889\n",
      "test, Batch: 3240/4779 Loss: 45.3924\n",
      "test, Batch: 3280/4779 Loss: 44.8421\n",
      "test, Batch: 3320/4779 Loss: 44.3036\n",
      "test, Batch: 3360/4779 Loss: 43.7786\n",
      "test, Batch: 3400/4779 Loss: 43.3124\n",
      "test, Batch: 3440/4779 Loss: 42.9300\n",
      "test, Batch: 3480/4779 Loss: 42.5851\n",
      "test, Batch: 3520/4779 Loss: 42.1527\n",
      "test, Batch: 3560/4779 Loss: 41.6823\n",
      "test, Batch: 3600/4779 Loss: 41.2216\n",
      "test, Batch: 3640/4779 Loss: 40.8277\n",
      "test, Batch: 3680/4779 Loss: 40.3863\n",
      "test, Batch: 3720/4779 Loss: 39.9538\n",
      "test, Batch: 3760/4779 Loss: 39.5313\n",
      "test, Batch: 3800/4779 Loss: 39.1552\n",
      "test, Batch: 3840/4779 Loss: 39.3889\n",
      "test, Batch: 3880/4779 Loss: 39.0563\n",
      "test, Batch: 3920/4779 Loss: 39.1969\n",
      "test, Batch: 3960/4779 Loss: 39.6597\n",
      "test, Batch: 4000/4779 Loss: 39.2653\n",
      "test, Batch: 4040/4779 Loss: 38.8789\n",
      "test, Batch: 4080/4779 Loss: 38.4998\n",
      "test, Batch: 4120/4779 Loss: 38.1710\n",
      "test, Batch: 4160/4779 Loss: 40.1166\n",
      "test, Batch: 4200/4779 Loss: 39.7368\n",
      "test, Batch: 4240/4779 Loss: 39.3639\n",
      "test, Batch: 4280/4779 Loss: 38.9992\n",
      "test, Batch: 4320/4779 Loss: 38.7841\n",
      "test, Batch: 4360/4779 Loss: 38.4306\n",
      "test, Batch: 4400/4779 Loss: 38.1658\n",
      "test, Batch: 4440/4779 Loss: 37.9076\n",
      "test, Batch: 4480/4779 Loss: 37.6643\n",
      "test, Batch: 4520/4779 Loss: 37.3792\n",
      "test, Batch: 4560/4779 Loss: 37.0531\n",
      "test, Batch: 4600/4779 Loss: 36.7326\n",
      "test, Batch: 4640/4779 Loss: 36.4201\n",
      "test, Batch: 4680/4779 Loss: 37.6377\n",
      "test, Batch: 4720/4779 Loss: 38.6043\n",
      "test, Batch: 4760/4779 Loss: 38.2858\n",
      "Epoch 13/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.1425\n",
      "train, Batch: 40/2390 Loss: 12.1511\n",
      "train, Batch: 80/2390 Loss: 18.1475\n",
      "train, Batch: 120/2390 Loss: 21.6820\n",
      "train, Batch: 160/2390 Loss: 19.4797\n",
      "train, Batch: 200/2390 Loss: 19.0972\n",
      "train, Batch: 240/2390 Loss: 19.6411\n",
      "train, Batch: 280/2390 Loss: 25.2355\n",
      "train, Batch: 320/2390 Loss: 22.8612\n",
      "train, Batch: 360/2390 Loss: 26.0376\n",
      "train, Batch: 400/2390 Loss: 23.7999\n",
      "train, Batch: 440/2390 Loss: 25.2232\n",
      "train, Batch: 480/2390 Loss: 23.2672\n",
      "train, Batch: 520/2390 Loss: 21.8226\n",
      "train, Batch: 560/2390 Loss: 21.3122\n",
      "train, Batch: 600/2390 Loss: 19.9809\n",
      "train, Batch: 640/2390 Loss: 21.3409\n",
      "train, Batch: 680/2390 Loss: 25.4252\n",
      "train, Batch: 720/2390 Loss: 24.3440\n",
      "train, Batch: 760/2390 Loss: 25.5082\n",
      "train, Batch: 800/2390 Loss: 26.3512\n",
      "train, Batch: 840/2390 Loss: 25.6676\n",
      "train, Batch: 880/2390 Loss: 25.0428\n",
      "train, Batch: 920/2390 Loss: 25.0780\n",
      "train, Batch: 960/2390 Loss: 24.2122\n",
      "train, Batch: 1000/2390 Loss: 24.6280\n",
      "train, Batch: 1040/2390 Loss: 26.5336\n",
      "train, Batch: 1080/2390 Loss: 26.9738\n",
      "train, Batch: 1120/2390 Loss: 29.2495\n",
      "train, Batch: 1160/2390 Loss: 29.1590\n",
      "train, Batch: 1200/2390 Loss: 29.4402\n",
      "train, Batch: 1240/2390 Loss: 28.9808\n",
      "train, Batch: 1280/2390 Loss: 28.2833\n",
      "train, Batch: 1320/2390 Loss: 28.1889\n",
      "train, Batch: 1360/2390 Loss: 28.9613\n",
      "train, Batch: 1400/2390 Loss: 29.4311\n",
      "train, Batch: 1440/2390 Loss: 29.0827\n",
      "train, Batch: 1480/2390 Loss: 28.5401\n",
      "train, Batch: 1520/2390 Loss: 28.2055\n",
      "train, Batch: 1560/2390 Loss: 27.6017\n",
      "train, Batch: 1600/2390 Loss: 27.1397\n",
      "train, Batch: 1640/2390 Loss: 26.6428\n",
      "train, Batch: 1680/2390 Loss: 26.1197\n",
      "train, Batch: 1720/2390 Loss: 25.8143\n",
      "train, Batch: 1760/2390 Loss: 26.3360\n",
      "train, Batch: 1800/2390 Loss: 25.8208\n",
      "train, Batch: 1840/2390 Loss: 25.3237\n",
      "train, Batch: 1880/2390 Loss: 25.7226\n",
      "train, Batch: 1920/2390 Loss: 25.2748\n",
      "train, Batch: 1960/2390 Loss: 24.7928\n",
      "train, Batch: 2000/2390 Loss: 25.0685\n",
      "train, Batch: 2040/2390 Loss: 24.6868\n",
      "train, Batch: 2080/2390 Loss: 25.2777\n",
      "train, Batch: 2120/2390 Loss: 24.9272\n",
      "train, Batch: 2160/2390 Loss: 24.4992\n",
      "train, Batch: 2200/2390 Loss: 24.6370\n",
      "train, Batch: 2240/2390 Loss: 25.1809\n",
      "train, Batch: 2280/2390 Loss: 25.5497\n",
      "train, Batch: 2320/2390 Loss: 25.8112\n",
      "train, Batch: 2360/2390 Loss: 25.9012\n",
      "test, Batch: 0/4779 Loss: 1.2022\n",
      "test, Batch: 40/4779 Loss: 14.6578\n",
      "test, Batch: 80/4779 Loss: 7.9502\n",
      "test, Batch: 120/4779 Loss: 132.7324\n",
      "test, Batch: 160/4779 Loss: 100.4813\n",
      "test, Batch: 200/4779 Loss: 81.4275\n",
      "test, Batch: 240/4779 Loss: 91.7011\n",
      "test, Batch: 280/4779 Loss: 79.2593\n",
      "test, Batch: 320/4779 Loss: 70.1234\n",
      "test, Batch: 360/4779 Loss: 62.4688\n",
      "test, Batch: 400/4779 Loss: 56.3770\n",
      "test, Batch: 440/4779 Loss: 51.3601\n",
      "test, Batch: 480/4779 Loss: 71.0443\n",
      "test, Batch: 520/4779 Loss: 66.3661\n",
      "test, Batch: 560/4779 Loss: 63.0675\n",
      "test, Batch: 600/4779 Loss: 59.4666\n",
      "test, Batch: 640/4779 Loss: 55.8134\n",
      "test, Batch: 680/4779 Loss: 52.6161\n",
      "test, Batch: 720/4779 Loss: 69.7219\n",
      "test, Batch: 760/4779 Loss: 66.3711\n",
      "test, Batch: 800/4779 Loss: 63.5440\n",
      "test, Batch: 840/4779 Loss: 60.8130\n",
      "test, Batch: 880/4779 Loss: 60.2389\n",
      "test, Batch: 920/4779 Loss: 57.6644\n",
      "test, Batch: 960/4779 Loss: 55.3052\n",
      "test, Batch: 1000/4779 Loss: 59.2610\n",
      "test, Batch: 1040/4779 Loss: 73.1532\n",
      "test, Batch: 1080/4779 Loss: 70.4888\n",
      "test, Batch: 1120/4779 Loss: 68.0092\n",
      "test, Batch: 1160/4779 Loss: 65.8641\n",
      "test, Batch: 1200/4779 Loss: 67.7200\n",
      "test, Batch: 1240/4779 Loss: 66.6652\n",
      "test, Batch: 1280/4779 Loss: 64.6160\n",
      "test, Batch: 1320/4779 Loss: 62.7735\n",
      "test, Batch: 1360/4779 Loss: 61.1081\n",
      "test, Batch: 1400/4779 Loss: 59.3891\n",
      "test, Batch: 1440/4779 Loss: 57.7686\n",
      "test, Batch: 1480/4779 Loss: 56.2367\n",
      "test, Batch: 1520/4779 Loss: 55.0563\n",
      "test, Batch: 1560/4779 Loss: 53.7928\n",
      "test, Batch: 1600/4779 Loss: 52.9569\n",
      "test, Batch: 1640/4779 Loss: 51.6920\n",
      "test, Batch: 1680/4779 Loss: 51.2189\n",
      "test, Batch: 1720/4779 Loss: 50.3087\n",
      "test, Batch: 1760/4779 Loss: 49.3629\n",
      "test, Batch: 1800/4779 Loss: 48.5746\n",
      "test, Batch: 1840/4779 Loss: 47.7484\n",
      "test, Batch: 1880/4779 Loss: 52.1373\n",
      "test, Batch: 1920/4779 Loss: 53.6314\n",
      "test, Batch: 1960/4779 Loss: 52.5570\n",
      "test, Batch: 2000/4779 Loss: 51.6239\n",
      "test, Batch: 2040/4779 Loss: 53.3852\n",
      "test, Batch: 2080/4779 Loss: 52.3806\n",
      "test, Batch: 2120/4779 Loss: 51.4691\n",
      "test, Batch: 2160/4779 Loss: 53.3435\n",
      "test, Batch: 2200/4779 Loss: 52.3929\n",
      "test, Batch: 2240/4779 Loss: 51.4782\n",
      "test, Batch: 2280/4779 Loss: 50.6824\n",
      "test, Batch: 2320/4779 Loss: 49.8263\n",
      "test, Batch: 2360/4779 Loss: 48.9992\n",
      "test, Batch: 2400/4779 Loss: 48.1982\n",
      "test, Batch: 2440/4779 Loss: 47.4680\n",
      "test, Batch: 2480/4779 Loss: 46.9931\n",
      "test, Batch: 2520/4779 Loss: 46.2643\n",
      "test, Batch: 2560/4779 Loss: 45.5565\n",
      "test, Batch: 2600/4779 Loss: 45.4822\n",
      "test, Batch: 2640/4779 Loss: 44.8819\n",
      "test, Batch: 2680/4779 Loss: 44.2288\n",
      "test, Batch: 2720/4779 Loss: 43.5925\n",
      "test, Batch: 2760/4779 Loss: 42.9768\n",
      "test, Batch: 2800/4779 Loss: 42.5655\n",
      "test, Batch: 2840/4779 Loss: 45.8952\n",
      "test, Batch: 2880/4779 Loss: 45.7634\n",
      "test, Batch: 2920/4779 Loss: 46.1692\n",
      "test, Batch: 2960/4779 Loss: 45.6056\n",
      "test, Batch: 3000/4779 Loss: 45.0108\n",
      "test, Batch: 3040/4779 Loss: 44.4962\n",
      "test, Batch: 3080/4779 Loss: 43.9321\n",
      "test, Batch: 3120/4779 Loss: 44.3125\n",
      "test, Batch: 3160/4779 Loss: 43.8256\n",
      "test, Batch: 3200/4779 Loss: 44.2695\n",
      "test, Batch: 3240/4779 Loss: 43.7349\n",
      "test, Batch: 3280/4779 Loss: 43.2155\n",
      "test, Batch: 3320/4779 Loss: 42.7798\n",
      "test, Batch: 3360/4779 Loss: 42.2827\n",
      "test, Batch: 3400/4779 Loss: 41.8567\n",
      "test, Batch: 3440/4779 Loss: 41.3825\n",
      "test, Batch: 3480/4779 Loss: 40.9588\n",
      "test, Batch: 3520/4779 Loss: 43.4605\n",
      "test, Batch: 3560/4779 Loss: 42.9841\n",
      "test, Batch: 3600/4779 Loss: 42.5565\n",
      "test, Batch: 3640/4779 Loss: 42.1004\n",
      "test, Batch: 3680/4779 Loss: 41.6541\n",
      "test, Batch: 3720/4779 Loss: 41.2169\n",
      "test, Batch: 3760/4779 Loss: 40.7902\n",
      "test, Batch: 3800/4779 Loss: 40.7369\n",
      "test, Batch: 3840/4779 Loss: 40.3226\n",
      "test, Batch: 3880/4779 Loss: 39.9161\n",
      "test, Batch: 3920/4779 Loss: 39.6235\n",
      "test, Batch: 3960/4779 Loss: 39.2342\n",
      "test, Batch: 4000/4779 Loss: 38.8875\n",
      "test, Batch: 4040/4779 Loss: 38.5404\n",
      "test, Batch: 4080/4779 Loss: 38.2104\n",
      "test, Batch: 4120/4779 Loss: 42.6527\n",
      "test, Batch: 4160/4779 Loss: 42.2513\n",
      "test, Batch: 4200/4779 Loss: 41.9588\n",
      "test, Batch: 4240/4779 Loss: 41.5713\n",
      "test, Batch: 4280/4779 Loss: 41.1925\n",
      "test, Batch: 4320/4779 Loss: 40.8211\n",
      "test, Batch: 4360/4779 Loss: 40.4559\n",
      "test, Batch: 4400/4779 Loss: 40.1212\n",
      "test, Batch: 4440/4779 Loss: 39.7710\n",
      "test, Batch: 4480/4779 Loss: 39.4666\n",
      "test, Batch: 4520/4779 Loss: 39.1248\n",
      "test, Batch: 4560/4779 Loss: 38.7898\n",
      "test, Batch: 4600/4779 Loss: 38.5768\n",
      "test, Batch: 4640/4779 Loss: 38.5465\n",
      "test, Batch: 4680/4779 Loss: 38.2480\n",
      "test, Batch: 4720/4779 Loss: 37.9326\n",
      "test, Batch: 4760/4779 Loss: 37.6232\n",
      "Epoch 14/14\n",
      "----------\n",
      "train, Batch: 0/2390 Loss: 0.6841\n",
      "train, Batch: 40/2390 Loss: 5.9556\n",
      "train, Batch: 80/2390 Loss: 21.8561\n",
      "train, Batch: 120/2390 Loss: 23.1541\n",
      "train, Batch: 160/2390 Loss: 43.2527\n",
      "train, Batch: 200/2390 Loss: 47.5187\n",
      "train, Batch: 240/2390 Loss: 40.3016\n",
      "train, Batch: 280/2390 Loss: 38.2950\n",
      "train, Batch: 320/2390 Loss: 35.6746\n",
      "train, Batch: 360/2390 Loss: 38.3716\n",
      "train, Batch: 400/2390 Loss: 34.6116\n",
      "train, Batch: 440/2390 Loss: 37.9030\n",
      "train, Batch: 480/2390 Loss: 36.9676\n",
      "train, Batch: 520/2390 Loss: 34.5975\n",
      "train, Batch: 560/2390 Loss: 33.2606\n",
      "train, Batch: 600/2390 Loss: 35.1729\n",
      "train, Batch: 640/2390 Loss: 36.0581\n",
      "train, Batch: 680/2390 Loss: 34.0152\n",
      "train, Batch: 720/2390 Loss: 32.8957\n",
      "train, Batch: 760/2390 Loss: 31.6234\n",
      "train, Batch: 800/2390 Loss: 30.6291\n",
      "train, Batch: 840/2390 Loss: 29.2830\n",
      "train, Batch: 880/2390 Loss: 29.7412\n",
      "train, Batch: 920/2390 Loss: 28.6632\n",
      "train, Batch: 960/2390 Loss: 28.5939\n",
      "train, Batch: 1000/2390 Loss: 27.7023\n",
      "train, Batch: 1040/2390 Loss: 26.8093\n",
      "train, Batch: 1080/2390 Loss: 26.0307\n",
      "train, Batch: 1120/2390 Loss: 25.3622\n",
      "train, Batch: 1160/2390 Loss: 24.6354\n",
      "train, Batch: 1200/2390 Loss: 24.2739\n",
      "train, Batch: 1240/2390 Loss: 24.4168\n",
      "train, Batch: 1280/2390 Loss: 24.0693\n",
      "train, Batch: 1320/2390 Loss: 24.0553\n",
      "train, Batch: 1360/2390 Loss: 23.4324\n",
      "train, Batch: 1400/2390 Loss: 22.8903\n",
      "train, Batch: 1440/2390 Loss: 22.3355\n",
      "train, Batch: 1480/2390 Loss: 22.5098\n",
      "train, Batch: 1520/2390 Loss: 22.2956\n",
      "train, Batch: 1560/2390 Loss: 22.5460\n",
      "train, Batch: 1600/2390 Loss: 22.7415\n",
      "train, Batch: 1640/2390 Loss: 24.0561\n",
      "train, Batch: 1680/2390 Loss: 23.7742\n",
      "train, Batch: 1720/2390 Loss: 23.2629\n",
      "train, Batch: 1760/2390 Loss: 23.0188\n",
      "train, Batch: 1800/2390 Loss: 23.6321\n",
      "train, Batch: 1840/2390 Loss: 23.6497\n",
      "train, Batch: 1880/2390 Loss: 23.8252\n",
      "train, Batch: 1920/2390 Loss: 24.5727\n",
      "train, Batch: 1960/2390 Loss: 24.1083\n",
      "train, Batch: 2000/2390 Loss: 23.8014\n",
      "train, Batch: 2040/2390 Loss: 23.3863\n",
      "train, Batch: 2080/2390 Loss: 24.0262\n",
      "train, Batch: 2120/2390 Loss: 23.7932\n",
      "train, Batch: 2160/2390 Loss: 23.6677\n",
      "train, Batch: 2200/2390 Loss: 23.3500\n",
      "train, Batch: 2240/2390 Loss: 24.4135\n",
      "train, Batch: 2280/2390 Loss: 24.1046\n",
      "train, Batch: 2320/2390 Loss: 25.0551\n",
      "train, Batch: 2360/2390 Loss: 24.7327\n",
      "test, Batch: 0/4779 Loss: 0.0016\n",
      "test, Batch: 40/4779 Loss: 0.1339\n",
      "test, Batch: 80/4779 Loss: 0.6484\n",
      "test, Batch: 120/4779 Loss: 25.2798\n",
      "test, Batch: 160/4779 Loss: 19.0327\n",
      "test, Batch: 200/4779 Loss: 18.8357\n",
      "test, Batch: 240/4779 Loss: 15.8317\n",
      "test, Batch: 280/4779 Loss: 14.2977\n",
      "test, Batch: 320/4779 Loss: 12.5331\n",
      "test, Batch: 360/4779 Loss: 11.1658\n",
      "test, Batch: 400/4779 Loss: 18.4712\n",
      "test, Batch: 440/4779 Loss: 16.8178\n",
      "test, Batch: 480/4779 Loss: 15.4324\n",
      "test, Batch: 520/4779 Loss: 14.2554\n",
      "test, Batch: 560/4779 Loss: 33.6916\n",
      "test, Batch: 600/4779 Loss: 31.4569\n",
      "test, Batch: 640/4779 Loss: 29.8587\n",
      "test, Batch: 680/4779 Loss: 37.4059\n",
      "test, Batch: 720/4779 Loss: 35.4926\n",
      "test, Batch: 760/4779 Loss: 33.6327\n",
      "test, Batch: 800/4779 Loss: 31.9586\n",
      "test, Batch: 840/4779 Loss: 43.5407\n",
      "test, Batch: 880/4779 Loss: 41.7448\n",
      "test, Batch: 920/4779 Loss: 40.2631\n",
      "test, Batch: 960/4779 Loss: 38.7644\n",
      "test, Batch: 1000/4779 Loss: 37.5452\n",
      "test, Batch: 1040/4779 Loss: 36.2530\n",
      "test, Batch: 1080/4779 Loss: 35.0264\n",
      "test, Batch: 1120/4779 Loss: 34.9753\n",
      "test, Batch: 1160/4779 Loss: 38.5654\n",
      "test, Batch: 1200/4779 Loss: 37.2847\n",
      "test, Batch: 1240/4779 Loss: 36.0861\n",
      "test, Batch: 1280/4779 Loss: 34.9662\n",
      "test, Batch: 1320/4779 Loss: 33.9172\n",
      "test, Batch: 1360/4779 Loss: 33.4890\n",
      "test, Batch: 1400/4779 Loss: 32.8135\n",
      "test, Batch: 1440/4779 Loss: 40.8013\n",
      "test, Batch: 1480/4779 Loss: 39.7027\n",
      "test, Batch: 1520/4779 Loss: 38.6626\n",
      "test, Batch: 1560/4779 Loss: 37.6760\n",
      "test, Batch: 1600/4779 Loss: 40.3496\n",
      "test, Batch: 1640/4779 Loss: 39.3689\n",
      "test, Batch: 1680/4779 Loss: 40.1549\n",
      "test, Batch: 1720/4779 Loss: 39.2248\n",
      "test, Batch: 1760/4779 Loss: 39.1483\n",
      "test, Batch: 1800/4779 Loss: 38.2879\n",
      "test, Batch: 1840/4779 Loss: 37.4605\n",
      "test, Batch: 1880/4779 Loss: 36.6674\n",
      "test, Batch: 1920/4779 Loss: 36.0117\n",
      "test, Batch: 1960/4779 Loss: 35.5963\n",
      "test, Batch: 2000/4779 Loss: 35.0452\n",
      "test, Batch: 2040/4779 Loss: 34.6965\n",
      "test, Batch: 2080/4779 Loss: 34.1323\n",
      "test, Batch: 2120/4779 Loss: 33.4971\n",
      "test, Batch: 2160/4779 Loss: 32.8806\n",
      "test, Batch: 2200/4779 Loss: 32.2863\n",
      "test, Batch: 2240/4779 Loss: 31.7750\n",
      "test, Batch: 2280/4779 Loss: 31.2201\n",
      "test, Batch: 2320/4779 Loss: 30.6843\n",
      "test, Batch: 2360/4779 Loss: 30.1670\n",
      "test, Batch: 2400/4779 Loss: 30.2598\n",
      "test, Batch: 2440/4779 Loss: 29.8523\n",
      "test, Batch: 2480/4779 Loss: 29.3763\n",
      "test, Batch: 2520/4779 Loss: 28.9896\n",
      "test, Batch: 2560/4779 Loss: 28.5388\n",
      "test, Batch: 2600/4779 Loss: 32.4925\n",
      "test, Batch: 2640/4779 Loss: 32.1226\n",
      "test, Batch: 2680/4779 Loss: 31.7018\n",
      "test, Batch: 2720/4779 Loss: 31.3622\n",
      "test, Batch: 2760/4779 Loss: 31.3444\n",
      "test, Batch: 2800/4779 Loss: 36.0417\n",
      "test, Batch: 2840/4779 Loss: 35.5357\n",
      "test, Batch: 2880/4779 Loss: 35.0446\n",
      "test, Batch: 2920/4779 Loss: 36.0904\n",
      "test, Batch: 2960/4779 Loss: 35.6695\n",
      "test, Batch: 3000/4779 Loss: 35.1966\n",
      "test, Batch: 3040/4779 Loss: 34.8493\n",
      "test, Batch: 3080/4779 Loss: 34.3986\n",
      "test, Batch: 3120/4779 Loss: 33.9600\n",
      "test, Batch: 3160/4779 Loss: 33.5319\n",
      "test, Batch: 3200/4779 Loss: 33.1157\n",
      "test, Batch: 3240/4779 Loss: 32.8329\n",
      "test, Batch: 3280/4779 Loss: 32.4353\n",
      "test, Batch: 3320/4779 Loss: 32.4137\n",
      "test, Batch: 3360/4779 Loss: 32.0289\n",
      "test, Batch: 3400/4779 Loss: 31.6538\n",
      "test, Batch: 3440/4779 Loss: 31.2874\n",
      "test, Batch: 3480/4779 Loss: 30.9293\n",
      "test, Batch: 3520/4779 Loss: 31.0349\n",
      "test, Batch: 3560/4779 Loss: 31.1593\n",
      "test, Batch: 3600/4779 Loss: 30.8704\n",
      "test, Batch: 3640/4779 Loss: 30.5329\n",
      "test, Batch: 3680/4779 Loss: 30.2827\n",
      "test, Batch: 3720/4779 Loss: 30.0140\n",
      "test, Batch: 3760/4779 Loss: 33.7916\n",
      "test, Batch: 3800/4779 Loss: 33.4892\n",
      "test, Batch: 3840/4779 Loss: 33.2434\n",
      "test, Batch: 3880/4779 Loss: 32.9030\n",
      "test, Batch: 3920/4779 Loss: 32.5683\n",
      "test, Batch: 3960/4779 Loss: 32.5201\n",
      "test, Batch: 4000/4779 Loss: 33.6750\n",
      "test, Batch: 4040/4779 Loss: 33.3432\n",
      "test, Batch: 4080/4779 Loss: 33.0177\n",
      "test, Batch: 4120/4779 Loss: 33.0162\n",
      "test, Batch: 4160/4779 Loss: 32.7006\n",
      "test, Batch: 4200/4779 Loss: 32.3913\n",
      "test, Batch: 4240/4779 Loss: 32.0873\n",
      "test, Batch: 4280/4779 Loss: 35.9240\n",
      "test, Batch: 4320/4779 Loss: 35.6256\n",
      "test, Batch: 4360/4779 Loss: 35.4444\n",
      "test, Batch: 4400/4779 Loss: 35.1236\n",
      "test, Batch: 4440/4779 Loss: 34.8083\n",
      "test, Batch: 4480/4779 Loss: 35.5349\n",
      "test, Batch: 4520/4779 Loss: 35.2252\n",
      "test, Batch: 4560/4779 Loss: 37.1684\n",
      "test, Batch: 4600/4779 Loss: 36.8913\n",
      "test, Batch: 4640/4779 Loss: 37.6338\n",
      "test, Batch: 4680/4779 Loss: 37.3156\n",
      "test, Batch: 4720/4779 Loss: 37.0007\n",
      "test, Batch: 4760/4779 Loss: 37.0667\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_second_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=2, dataloaders=dataloaders, dataset_sizes=dataset_sizes, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_pupil(model, dataloaders['test'], device, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
